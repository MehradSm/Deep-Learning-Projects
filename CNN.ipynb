{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building and Training a ConvNet on SVHN\n",
    "(25 points)\n",
    "\n",
    "First we provide demo code that trains a convolutional network on the [SVHN Dataset](http://ufldl.stanford.edu/housenumbers/).. \n",
    "\n",
    "You will need to download   __Format 2__ from the link above.\n",
    "- Create a directory named `svhn_mat/` in the working directory. Or, you can create it anywhere you want, but change the path in `svhn_dataset_generator` to match it.\n",
    "- Download `train_32x32.mat` and `test_32x32.mat` to this directory.\n",
    "- `extra_32x32.mat` is NOT needed.\n",
    "- You may find the `wget` command useful for downloading on linux. \n",
    "\n",
    "\n",
    "\n",
    "The following defines a generator for the SVHN Dataset, yielding the next batch every time next is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "\n",
    "from six.moves import range\n",
    "import read_data\n",
    "\n",
    "\n",
    "print_every = 20\n",
    "epoch_n = 50\n",
    "\n",
    "################################## Loading Date ###############################\n",
    "\n",
    "@read_data.restartable\n",
    "def svhn_dataset_generator(dataset_name, batch_size):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    path = './svhn_mat/' # path to the SVHN dataset you will download in Q1.1\n",
    "    file_name = '%s_32x32.mat' % dataset_name\n",
    "    file_dict = scipy.io.loadmat(os.path.join(path, file_name))\n",
    "    X_all = file_dict['X'].transpose((3, 0, 1, 2))\n",
    "    y_all = file_dict['y']\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    y_all_padded[y_all_padded == 10] = 0\n",
    "    \n",
    "    for slice_i in range(int(math.ceil(data_len / batch_size))):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines the CovNet Model. It has two identical conv layers with 32 5x5 convlution filters, followed by a fully-connected layer to output the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### ConvNet Model #############################\n",
    "\n",
    "def cnn_map(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # stride [2,2]\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # stride [2,2]\n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Training SVHN Net\n",
    "Now we train a `cnn_map` net on Format 2 of the SVHN Dataset. We will call this \"SVHN net\". \n",
    "\n",
    "**Note:** training will take a while, so you might want to use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Training Model #############################\n",
    "dataset_generators = {\n",
    "        'train': svhn_dataset_generator('train', 256),\n",
    "        'test': svhn_dataset_generator('test', 256)\n",
    "}\n",
    "\n",
    "with tf.device(\"/gpu:0\"):  # use gpu:0 if on GPU\n",
    "   sess = tf.InteractiveSession()\n",
    "   x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "   y_ = tf.placeholder(tf.int32, [None])\n",
    "   y_logits = cnn_map(x_)\n",
    "           \n",
    "   y_dict = dict(labels=y_, logits=y_logits)\n",
    "   losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "   cross_entropy_loss = tf.reduce_mean(losses)\n",
    "   trainer = tf.train.AdamOptimizer()\n",
    "   train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "   y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "   correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "   sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "   for epoch_i in range(epoch_n):\n",
    "       for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "           train_op.run(feed_dict={x_: data_batch[0], y_: data_batch[1]})  \n",
    "           if iter_i % print_every == 0:\n",
    "                collect = []\n",
    "                arrloss = []\n",
    "                for test_batch in dataset_generators['test']:\n",
    "                    collect.append(accuracy.eval(feed_dict={x_: test_batch[0], y_: test_batch[1]}))\n",
    "                    arrloss.append(cross_entropy_loss.eval(feed_dict={x_: test_batch[0], y_: test_batch[1]}))\n",
    "\n",
    "                test_acc = np.mean(collect, axis=0)\n",
    "                test_loss = np.mean(arrloss, axis=0)\n",
    "                print(\"iter %d    epoch %d   test accuracy %.3f   loss %.3f\" %(iter_i, epoch_i, test_acc, test_loss) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3 SVHN Net Variations\n",
    "Now we vary the structure of the network. To keep things simple, we still use  two identical conv layers, but vary their parameters. \n",
    "\n",
    "Report the final test accuracy on 3 different number of filters, and 3 different number of strides. Each time when you vary one parameter, keep the other fixed at the original value.\n",
    "\n",
    "|Stride (with filters=32)|Accuracy|\n",
    "|--|-------------------------------|\n",
    "| 2 | 0.828 |\n",
    "| 3 | 0.840 |\n",
    "| 4 | 0.805 |\n",
    "\n",
    "|Filters (with stride=2)|Accuracy|\n",
    "|--|-------------------------------|\n",
    "| 20 | 0.830 |\n",
    "|  30| 0.829 |\n",
    "|  40| 0.820 |\n",
    "\n",
    "A template for one sample modification is given below. \n",
    "\n",
    "**Note:** you're welcome to decide how many training epochs to use, if that gets you the same results but faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 20\n",
    "epoch_n = 30\n",
    "Numfilt  =  32\n",
    "Numstride  = 3\n",
    "\n",
    "################################### ConvNet Model #############################\n",
    "\n",
    "def cnn_map(x_,Numfilt,Numstride):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters = Numfilt,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=Numstride)  \n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=Numfilt, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=Numstride)  \n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "\n",
    "    \t\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "    \n",
    "################################## Training Model ########################################\n",
    "\n",
    "dataset_generators = {\n",
    "        'train': svhn_dataset_generator('train', 256),\n",
    "        'test': svhn_dataset_generator('test', 256)\n",
    "}\n",
    "\n",
    "with tf.device(\"/gpu:0\"):  # use gpu:0 if on GPU\n",
    "   sess = tf.InteractiveSession()\n",
    "   x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "   y_ = tf.placeholder(tf.int32, [None])\n",
    "   y_logits = cnn_map(x_,Numfilt,Numstride) \n",
    "           \n",
    "   y_dict = dict(labels=y_, logits=y_logits)\n",
    "   losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "   cross_entropy_loss = tf.reduce_mean(losses)\n",
    "   trainer = tf.train.AdamOptimizer()\n",
    "   train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "   y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "   correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "   sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "   for epoch_i in range(epoch_n):\n",
    "        for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "            train_op.run(feed_dict={x_: data_batch[0], y_: data_batch[1]})  \n",
    "\n",
    "            if iter_i % print_every == 0:\n",
    "                collect = []\n",
    "                arrloss = []\n",
    "            \t\n",
    "                for test_batch in dataset_generators['test']:\n",
    "                    collect.append(accuracy.eval(feed_dict={x_: test_batch[0], y_: test_batch[1]}))\n",
    "                    arrloss.append(cross_entropy_loss.eval(feed_dict={x_: test_batch[0], y_: test_batch[1]}))\n",
    "  \n",
    "                test_acc = np.mean(collect, axis=0)\n",
    "                test_loss = np.mean(arrloss, axis=0)\n",
    "                print(\"iter %d    epoch %d   test accuracy %.3f   loss %.3f\" %(iter_i, epoch_i, test_acc, test_loss) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Saving and Reloading Model Weights\n",
    "(25 points)\n",
    "\n",
    "In this section you learn to save the weights of a trained model, and to load the weights of a saved model. This is really useful when we would like to load an already trained model in order to continue training or to fine-tune it. Often times we save “snapshots” of the trained model as training progresses in case the training is interrupted, or in case we would like to fall back to an earlier model, this is called snapshot saving.\n",
    "\n",
    "### Q2.1 Defining another network\n",
    "Define a network with a slightly different structure in `def cnn_expanded(x_)` below. `cnn_expanded` is an expanded version of `cnn_model`. \n",
    "It should have: \n",
    "- a different size of kernel for the last convolutional layer, \n",
    "- followed by one additional convolutional layer, and \n",
    "- followed by one additional pooling layer.\n",
    "\n",
    "The last fully-connected layer will stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_map(x_,NumCovLayer,Numfilt,Numstride,KernelSize):\n",
    "\n",
    "    for i in range(NumCovLayer):\n",
    "        with tf.name_scope('layer'+str(i)):\n",
    "            conv = tf.layers.conv2d(\n",
    "                    inputs=x_,\n",
    "                    filters = Numfilt,  \n",
    "                    kernel_size=[KernelSize[i], KernelSize[i]],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.relu)\n",
    "    \n",
    "            pool = tf.layers.max_pooling2d(inputs=conv,pool_size=[2, 2],strides=Numstride)    \n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool, scope='poolflat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Saving and Loading Weights\n",
    "`new_train_model()` below has two additional parameters `save_model=False, load_model=False` than `train_model` defined previously. Modify `new_train_model()` such that it would \n",
    "- save weights after the training is complete if `save_model` is `True`, and\n",
    "- load weights on start-up before training if `load_model` is `True`.\n",
    "\n",
    "*Hint:*  take a look at the docs for `tf.train.Saver()` here: https://www.tensorflow.org/api_docs/python/tf/train/Saver#__init__. You probably will be specifying the first argument `var_list` to accomplish this question.\n",
    "\n",
    "Note: if you are unable to load weights into `cnn_expanded` network, use `cnn_map` in order to continue the assingment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_every = 20\n",
    "epoch_n = 25\n",
    "Numfilt  =  32\n",
    "Numstride  = 3\n",
    "KernelSize = [5,5,3,3]\n",
    "NumConvLayer = 4\n",
    "\n",
    "load_model= True\n",
    "save_model = False\n",
    "change_model = True\n",
    "\n",
    "LoadFile = \"weight/q22.ckpt\"\n",
    "SaveFile = \"weight/q22.ckpt\" \n",
    "\n",
    "################################## Loading Date ###############################\n",
    "\n",
    "@read_data.restartable\n",
    "def svhn_dataset_generator(dataset_name, batch_size):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    path = './svhn_mat/' # path to the SVHN dataset you will download in Q1.1\n",
    "    file_name = '%s_32x32.mat' % dataset_name\n",
    "    file_dict = scipy.io.loadmat(os.path.join(path, file_name))\n",
    "    X_all = file_dict['X'].transpose((3, 0, 1, 2))\n",
    "    y_all = file_dict['y']\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    y_all_padded[y_all_padded == 10] = 0\n",
    "    \n",
    "    for slice_i in range(int(math.ceil(data_len / batch_size))):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch, y_batch\n",
    "        \n",
    "        \n",
    "################################### ConvNet Model #############################\n",
    "\n",
    "def cnn_map(x_,NumConvLayer,Numfilt,Numstride,KernelSize):\n",
    "\n",
    "    filt = []\n",
    "    bias = []\n",
    "    for i in range(NumConvLayer):\n",
    "        with tf.variable_scope('layer'+str(i)):\n",
    "            conv = tf.layers.conv2d(\n",
    "                    inputs=x_,\n",
    "                    filters = Numfilt,  \n",
    "                    kernel_size=[KernelSize[i], KernelSize[i]],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.relu,\n",
    "                    name = \"conv\")\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            filt.append(tf.get_variable(name = 'conv/kernel'))\n",
    "            bias.append(tf.get_variable(name = 'conv/bias'))\n",
    "    \n",
    "            pool = tf.layers.max_pooling2d(inputs=conv, \n",
    "                                           pool_size=[2, 2], \n",
    "                                           strides=Numstride)   \n",
    "        \n",
    "        pool_flat = tf.contrib.layers.flatten(pool, scope='poolflat')\n",
    "        dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(inputs=dense, units=10)\n",
    "        output = [filt, bias, logits]\n",
    "    return output\n",
    "    \n",
    "################################## Training Model ########################################\n",
    "\n",
    "dataset_generators = {\n",
    "        'train': svhn_dataset_generator('train', 256),\n",
    "        'test': svhn_dataset_generator('test', 256)\n",
    "}\n",
    "\n",
    "with tf.device(\"/gpu:0\"):  # use gpu:0 if on GPU\n",
    "    x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y_ = tf.placeholder(tf.int32, [None])\n",
    "    filt, bias, y_logits = cnn_map(x_,NumConvLayer,Numfilt,Numstride,KernelSize)\n",
    "           \n",
    "    y_dict = dict(labels=y_, logits=y_logits)\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "    cross_entropy_loss = tf.reduce_mean(losses)\n",
    "    trainer = tf.train.AdamOptimizer()\n",
    "    train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "    y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "    correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()  \n",
    "if change_model == True :\n",
    "    shared_variables = {\"layer0/conv/kernel\":filt[0],\n",
    "                        \"layer0/conv/bias\":bias[0]}\n",
    "    saver = tf.train.Saver(shared_variables)\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "if load_model==True:\n",
    "    saver.restore(sess,LoadFile)\n",
    "    print('************** Weight is loaded ***************')\n",
    "   \t\t\n",
    "for epoch_i in range(epoch_n):\n",
    "    for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "        train_op.run(feed_dict={x_: data_batch[0], y_: data_batch[1]})  \n",
    "\n",
    "        if iter_i % print_every == 0:\n",
    "            collect = []\n",
    "            arrloss = []\n",
    "            \t\n",
    "            for test_batch in dataset_generators['test']:\n",
    "                collect.append(accuracy.eval(feed_dict={x_: test_batch[0], y_: test_batch[1]}))\n",
    "                arrloss.append(cross_entropy_loss.eval(feed_dict={x_: test_batch[0], y_: test_batch[1]}))\n",
    "                    \n",
    "                    \n",
    "                            \n",
    "            test_acc = np.mean(collect, axis=0)\n",
    "            test_loss = np.mean(arrloss, axis=0)\n",
    "            print(\"iter %d    epoch %d   test accuracy %.3f   loss %.3f\" %(iter_i, epoch_i, test_acc, test_loss) )\n",
    "                \n",
    "if save_model==True:\n",
    "    path = saver.save(sess,SaveFile)\n",
    "    print('***************** Weight is saved ****************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fine-tuning a Pre-trained Network on CIFAR-10\n",
    "(20 points)\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is another popular benchmark for image classification.\n",
    "We provide you with modified verstion of the file cifar10.py from [https://github.com/Hvass-Labs/TensorFlow-Tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import read_cifar10 as cf10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a generator for the CIFAR-10 Dataset, yielding the next batch every time next is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@read_data.restartable\n",
    "def cifar10_dataset_generator(dataset_name, batch_size, restrict_size=1000):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    X_all_unrestricted, y_all = (cf10.load_training_data() if dataset_name == 'train'\n",
    "                                 else cf10.load_test_data())\n",
    "    \n",
    "    actual_restrict_size = restrict_size if dataset_name == 'train' else int(1e10)\n",
    "    X_all = X_all_unrestricted[:actual_restrict_size]\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    \n",
    "    for slice_i in range(math.ceil(data_len / batch_size)):\n",
    "        idx = slice_i * batch_size\n",
    "        #X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]*255  # bugfix: thanks Zezhou Sun!\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch.astype(np.uint8), y_batch.astype(np.uint8)\n",
    "\n",
    "cifar10_dataset_generators = {\n",
    "    'train': cifar10_dataset_generator('train', 1000),\n",
    "    'test': cifar10_dataset_generator('test', -1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1 Fine-tuning\n",
    "Let's fine-tune SVHN net on **1000 examples** from CIFAR-10. \n",
    "Compare test accuracies of the following scenarios: \n",
    "  - Training `cnn_map` from scratch on the 1000 CIFAR-10 examples\n",
    "  - Fine-tuning SVHN net (`cnn_map` trained on SVHN dataset) on 1000 exampes from CIFAR-10. Use `new_train_model()` defined above to load SVHN net weights, but train on the CIFAR-10 examples.\n",
    "  \n",
    "**Important:** please do not change the `restrict_size=1000` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print_every = 20\n",
    "epoch_n = 500\n",
    "Numfilt  =  32\n",
    "Numstride  = 2\n",
    "KernelSize = [5,5,3,3]\n",
    "NumConvLayer = 4\n",
    "\n",
    "load_model= True\n",
    "save_model = True\n",
    "change_model = True\n",
    "\n",
    "LoadFile = \"weight/q22.ckpt\"\n",
    "SaveFile = \"Q3/q3.ckpt\"\n",
    "\n",
    "################################## Loading Date ###############################\n",
    "\n",
    "@read_data.restartable\n",
    "def cifar10_dataset_generator(dataset_name, batch_size, restrict_size=1000):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    X_all_unrestricted, y_all = (cf10.load_training_data() if dataset_name == 'train'\n",
    "                                 else cf10.load_test_data())\n",
    "    \n",
    "    actual_restrict_size = restrict_size if dataset_name == 'train' else int(1e10)\n",
    "    X_all = X_all_unrestricted[:actual_restrict_size]\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    \n",
    "    for slice_i in range(math.ceil(data_len / batch_size)):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]*255  \n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch.astype(np.uint8), y_batch.astype(np.uint8)\n",
    "        \n",
    "################################### ConvNet Model #############################\n",
    "\n",
    "def cnn_map(x_,NumConvLayer,Numfilt,Numstride,KernelSize):\n",
    "\n",
    "    filters = []\n",
    "    bias = []\n",
    "    for i in range(NumConvLayer):\n",
    "        with tf.variable_scope('layer'+str(i)):\n",
    "            conv = tf.layers.conv2d(\n",
    "                    inputs=x_,\n",
    "                    filters = Numfilt,  \n",
    "                    kernel_size=[KernelSize[i], KernelSize[i]],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.relu,\n",
    "                    name = \"conv\")\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            filters.append(tf.get_variable(name = 'conv/kernel'))\n",
    "            bias.append(tf.get_variable(name = 'conv/bias'))\n",
    "    \n",
    "            pool = tf.layers.max_pooling2d(inputs=conv, \n",
    "                                           pool_size=[2, 2], \n",
    "                                           strides=Numstride)   \n",
    "        \n",
    "        pool_flat = tf.contrib.layers.flatten(pool, scope='poolflat')\n",
    "        dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(inputs=dense, units=10)\n",
    "        output = [filters, bias, logits]\n",
    "    return output\n",
    "    \n",
    "################################## Training Model #############################\n",
    "\n",
    "cifar10_dataset_generators = {\n",
    "    'train': cifar10_dataset_generator('train', 1000),\n",
    "    'test': cifar10_dataset_generator('test', -1)\n",
    "}\n",
    "\n",
    "with tf.device(\"/gpu:0\"):  # use gpu:0 if on GPU\n",
    "    x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y_ = tf.placeholder(tf.int32, [None])\n",
    "    filters, bias, y_logits = cnn_map(x_,NumConvLayer,Numfilt,Numstride,KernelSize)\n",
    "           \n",
    "    y_dict = dict(labels=y_, logits=y_logits)\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "    cross_entropy_loss = tf.reduce_mean(losses)\n",
    "    trainer = tf.train.AdamOptimizer()\n",
    "    train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "    y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "    correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()  \n",
    "if change_model == True :\n",
    "    shared_variables = {\"layer0/conv/kernel\":filters[0],\n",
    "                        \"layer0/conv/bias\":bias[0]}\n",
    "    saver = tf.train.Saver(shared_variables)\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "if load_model==True:\n",
    "    saver.restore(sess,LoadFile)\n",
    "    print('**************** Weight is loaded *****************')\n",
    "   \t\t\n",
    "for epoch_i in range(epoch_n):\n",
    "    for iter_i, data_batch in enumerate(cifar10_dataset_generators['train']):\n",
    "        train_op.run(feed_dict={x_: data_batch[0], y_: data_batch[1]})  \n",
    "\n",
    "        if iter_i % print_every == 0:\n",
    "            collect = []\n",
    "            arrloss = []\n",
    "            \t\n",
    "            for test_batch in cifar10_dataset_generators['test']:\n",
    "                collect.append(accuracy.eval(feed_dict={x_: test_batch[0], y_: test_batch[1]}))\n",
    "                arrloss.append(cross_entropy_loss.eval(feed_dict={x_: test_batch[0], y_: test_batch[1]}))\n",
    "                    \n",
    "                    \n",
    "                            \n",
    "            test_acc = np.mean(collect, axis=0)\n",
    "            test_loss = np.mean(arrloss, axis=0)\n",
    "            print(\"iter %d    epoch %d   test accuracy %.3f   loss %.3f\" %(iter_i, epoch_i, test_acc, test_loss) )\n",
    "                \n",
    "if save_model==True:\n",
    "    path = saver.save(sess,SaveFile)\n",
    "    print('***************** Weight is saved *******************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I send the results in the Q4 file. **  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: TensorBoard\n",
    "(30 points)\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is a very helpful tool for visualization of neural networks. \n",
    "\n",
    "### Q4.1 Plotting\n",
    "Present at least one visualization for each of the following:\n",
    "  - Filters\n",
    "  - Loss\n",
    "  - Accuracy\n",
    "\n",
    "Modify code you have wrote above to also have summary writers. To  run tensorboard, the command is `tensorboard --logdir=path/to/your/log/directory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_every = 20\n",
    "epoch_n = 50\n",
    "Numfilt  =  32\n",
    "Numstride  = 2\n",
    "KernelSize = [5,5,3]\n",
    "NumConvLayer = 3\n",
    "\n",
    "\n",
    "\n",
    "################################## Loading Date ###############################\n",
    "\n",
    "@read_data.restartable\n",
    "def svhn_dataset_generator(dataset_name, batch_size):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    path = './svhn_mat/' # path to the SVHN dataset you will download in Q1.1\n",
    "    file_name = '%s_32x32.mat' % dataset_name\n",
    "    file_dict = scipy.io.loadmat(os.path.join(path, file_name))\n",
    "    X_all = file_dict['X'].transpose((3, 0, 1, 2))\n",
    "    y_all = file_dict['y']\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    y_all_padded[y_all_padded == 10] = 0\n",
    "    \n",
    "    for slice_i in range(int(math.ceil(data_len / batch_size))):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch, y_batch\n",
    "        \n",
    "        \n",
    "################################### ConvNet Model #############################\n",
    "\n",
    "def cnn_map(x_,NumConvLayer,Numfilt,Numstride,KernelSize):\n",
    "\n",
    "    filt = []\n",
    "    bias = []\n",
    "    for i in range(NumConvLayer):\n",
    "        with tf.variable_scope('layer'+str(i)):\n",
    "            conv = tf.layers.conv2d(\n",
    "                    inputs=x_,\n",
    "                    filters = Numfilt,  \n",
    "                    kernel_size=[KernelSize[i], KernelSize[i]],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.relu,\n",
    "                    name = \"conv\")\n",
    "            \n",
    "            \n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            filt.append(tf.get_variable(name = 'conv/kernel'))\n",
    "            bias.append(tf.get_variable(name = 'conv/bias'))\n",
    "            \n",
    "            \n",
    "    \n",
    "            pool = tf.layers.max_pooling2d(inputs=conv, \n",
    "                                           pool_size=[2, 2], \n",
    "                                           strides=Numstride)   \n",
    "        \n",
    "        pool_flat = tf.contrib.layers.flatten(pool, scope='poolflat')\n",
    "        dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(inputs=dense, units=10)\n",
    "        output = [filt, bias, logits]\n",
    "    return output\n",
    "    \n",
    "################################## Training Model ########################################\n",
    "\n",
    "dataset_generators = {\n",
    "        'train': svhn_dataset_generator('train', 256),\n",
    "        'test': svhn_dataset_generator('test', 256)\n",
    "}\n",
    "\n",
    "with tf.device(\"/gpu:0\"):  # use gpu:0 if on GPU\n",
    "    x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y_ = tf.placeholder(tf.int32, [None])\n",
    "    filt, bias, y_logits = cnn_map(x_,NumConvLayer,Numfilt,Numstride,KernelSize)\n",
    "           \n",
    "    y_dict = dict(labels=y_, logits=y_logits)\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "\n",
    "    cross_entropy_loss = tf.reduce_mean(losses)\n",
    "    trainer = tf.train.AdamOptimizer()\n",
    "    train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "    y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "    correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "with tf.device(\"/cpu:0\"):    \n",
    "    tf.summary.scalar('accurac',accuracy)\n",
    "    tf.summary.scalar('loss',cross_entropy_loss)\n",
    "    for i in range(NumConvLayer):\n",
    "        image_reshape = tf.reshape(filt[i], [-1, KernelSize[i], KernelSize[i], 3])\n",
    "        tf.summary.image('filter'+str(i), image_reshape, Numfilt)\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('summary_q4/')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch_i in range(epoch_n):\n",
    "    for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "        train_op.run(feed_dict={x_: data_batch[0], y_: data_batch[1]})  \n",
    "\n",
    "        if iter_i % print_every == 0:\n",
    "            collect = []\n",
    "            arrloss = []\n",
    "            for test_batch in dataset_generators['test']:\n",
    "                collect.append(accuracy.eval(feed_dict={x_: test_batch[0], y_: test_batch[1]}))\n",
    "                arrloss.append(cross_entropy_loss.eval(feed_dict={x_: test_batch[0], y_: test_batch[1]}))\n",
    "                summary= sess.run(merged, feed_dict={x_: test_batch[0], y_: test_batch[1]}) \n",
    "                writer.add_summary(summary, epoch_i*280 + iter_i)\n",
    "            \n",
    "        \n",
    "                    \n",
    "                            \n",
    "            test_acc = np.mean(collect, axis=0)\n",
    "            test_loss = np.mean(arrloss, axis=0)\n",
    "            print(\"iter %d    epoch %d   test accuracy %.3f   loss %.3f\" %(iter_i, epoch_i, test_acc, test_loss) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Bonus\n",
    "(20 points)\n",
    "\n",
    "### Q5.1 SVHN Net ++\n",
    "Improve the accuracy of SVHN Net beyond that of the provided demo: SVHN Net ++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 20\n",
    "epoch_n = 50\n",
    "Numfilt  =  32\n",
    "Numstride  = [2,2,4,4]\n",
    "KernelSize = [3,3,5,5]\n",
    "NumConvLayer = 4\n",
    "drop_out_ = 0.7;\n",
    "\n",
    "load_model= False\n",
    "save_model = True\n",
    "change_model = False\n",
    "\n",
    "LoadFile = \"Q5/q5.ckpt\"\n",
    "SaveFile = \"Q5/q5.ckpt\" \n",
    "\n",
    "################################## Loading Date ###############################\n",
    "\n",
    "@read_data.restartable\n",
    "def svhn_dataset_generator(dataset_name, batch_size):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    path = './svhn_mat/' # path to the SVHN dataset you will download in Q1.1\n",
    "    file_name = '%s_32x32.mat' % dataset_name\n",
    "    file_dict = scipy.io.loadmat(os.path.join(path, file_name))\n",
    "    X_all = file_dict['X'].transpose((3, 0, 1, 2))\n",
    "    y_all = file_dict['y']\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    y_all_padded[y_all_padded == 10] = 0\n",
    "    \n",
    "    for slice_i in range(int(math.ceil(data_len / batch_size))):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch, y_batch\n",
    "        \n",
    "        \n",
    "################################### ConvNet Model #############################\n",
    "\n",
    "def cnn_map(x_,NumConvLayer,Numfilt,Numstride,KernelSize,drop_out):\n",
    "\n",
    "    filters = []\n",
    "    bias = []\n",
    "    for i in range(NumConvLayer):\n",
    "        with tf.variable_scope('layer'+str(i)):\n",
    "            conv = tf.layers.conv2d(\n",
    "                    inputs=x_,\n",
    "                    filters = Numfilt,  \n",
    "                    kernel_size=[KernelSize[i], KernelSize[i]],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.relu,\n",
    "                    name = \"conv\")\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            droped_conv = tf.nn.dropout(conv, drop_out)\n",
    "            filters.append(tf.get_variable(name = 'conv/kernel'))\n",
    "            bias.append(tf.get_variable(name = 'conv/bias'))\n",
    "    \n",
    "            pool = tf.layers.max_pooling2d(inputs=droped_conv, \n",
    "                                           pool_size=[2, 2], \n",
    "                                           strides=Numstride[i])   \n",
    "        \n",
    "        pool_flat = tf.contrib.layers.flatten(pool, scope='poolflat')\n",
    "        dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "        droped_dense = tf.nn.dropout(dense, drop_out)\n",
    "        logits = tf.layers.dense(inputs=droped_dense, units=10)\n",
    "        output = [filters, bias, logits]\n",
    "    return output\n",
    "    \n",
    "################################## Training Model ########################################\n",
    "\n",
    "dataset_generators = {\n",
    "        'train': svhn_dataset_generator('train', 512),\n",
    "        'test': svhn_dataset_generator('test', 512)\n",
    "}\n",
    "\n",
    "with tf.device(\"/gpu:0\"):  # use gpu:0 if on GPU\n",
    "    x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y_ = tf.placeholder(tf.int32, [None])\n",
    "    drop_out = tf.placeholder(tf.float32)\n",
    "    filters, bias, y_logits = cnn_map(x_,NumConvLayer,Numfilt,Numstride,KernelSize, drop_out)\n",
    "           \n",
    "    y_dict = dict(labels=y_, logits=y_logits)\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "    cross_entropy_loss = tf.reduce_mean(losses)\n",
    "    trainer = tf.train.AdamOptimizer()\n",
    "    train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "    y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "    correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()  \n",
    "if change_model == True :\n",
    "    shared_variables = {\"layer0/conv/kernel\":filters[0],\n",
    "                        \"layer0/conv/bias\":bias[0]}\n",
    "    saver = tf.train.Saver(shared_variables)\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "if load_model==True:\n",
    "    saver.restore(sess,LoadFile)\n",
    "    print('Weight is loaded')\n",
    "\n",
    "for epoch_i in range(epoch_n):\n",
    "    for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "        train_op.run(feed_dict={x_: data_batch[0], y_: data_batch[1], drop_out : drop_out_})  \n",
    "\n",
    "        if iter_i % print_every == 0:\n",
    "            collect = []\n",
    "            arrloss = []\n",
    "            \t\n",
    "            for test_batch in dataset_generators['test']:\n",
    "                collect.append(accuracy.eval(feed_dict={x_: test_batch[0], y_: test_batch[1],drop_out : 1.0}))\n",
    "                arrloss.append(cross_entropy_loss.eval(feed_dict={x_: test_batch[0], y_: test_batch[1], drop_out : 1.0}))\n",
    "                    \n",
    "                    \n",
    "                            \n",
    "            test_acc = np.mean(collect, axis=0)\n",
    "            test_loss = np.mean(arrloss, axis=0)\n",
    "            print(\"iter %d    epoch %d   test accuracy %.3f   loss %.3f\" %(iter_i, epoch_i, test_acc, test_loss) )\n",
    "                \n",
    "if save_model==True:\n",
    "    path = saver.save(sess,SaveFile)\n",
    "    print('Weight is saved')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
