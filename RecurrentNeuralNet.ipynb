{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RNN Example\n",
    "\n",
    "(10 points)\n",
    "\n",
    "In this example we train an RNN to infer the parameters of a simple dynamical system.\n",
    "First, we simulate a dynamical system with known parameters (random numbers), and use it to generate outputs. Then, we train an RNN on the generated datapoints, attempting to infer the original parameters.\n",
    "You are expected to run and study the provided code, which will be helpful for the second part (implementing LSTM).\n",
    "\n",
    "1. We define a discretized [dynamical system](https://en.wikipedia.org/wiki/Dynamical_system).\n",
    "At each discrete time $t$, the system observes input $x_t$. The system maintains some \"state\" $h_t$, which will be updated over time.\n",
    "Specifically, the states are updated by the following rule: $h_t = \\max(0, 1-(Wx_t+h_{t-1}))$, where $W$ is a parameter matrix.\n",
    "2. In this example, the input data $\\{x_t\\}$ is randomly generated, and the weight matrix $W$ is also drawn randomly. The system starts from an initial hidden state $h_0$, and runs for $t=1,\\ldots,T$.\n",
    "3. Given the sequence of states $\\{h_1,\\ldots,h_T\\}$, we would like to infer the weights $W$.\n",
    "\n",
    "To start with, the following code segment generates and displays the data.\n",
    "Refer to `data_generator.py` for details of data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable name, shape, min, max: \n",
      "h0 (10,) -0.227338038799 2.18803183795\n",
      "w (10, 10) -3.15093751889 1.95340496211\n",
      "x (100, 10) -3.15554537384 3.36526071537\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9c09b7898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we present sample data generated \n",
    "import numpy as np\n",
    "import data_generator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(h_0, w), x, h = data_generator._build_rnn_testdata_matrix()\n",
    "\n",
    "print('variable name, shape, min, max: ')\n",
    "for v, name in zip([h_0, w, x], ['h0', 'w', 'x']):\n",
    "    print(name, v.shape, np.min(v), np.max(v))\n",
    "norm_x_t = np.sum(x*2, axis=1)\n",
    "plt.title('||x(t)||')\n",
    "plt.plot(np.arange(norm_x_t.shape[0]), norm_x_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the dynamical system with an RNN. We would like the state generated by the RNN to match the actual observed, and we use L2 loss for this purpose.\n",
    "This RNN is a *regression* model since it outputs real values.\n",
    "\n",
    "Below, `build_rnn_regression_model()` gives the model definition, and `train_rnn_with_noise()` generates a batch of data and then runs training on it. \n",
    "We corrupt the data generation process with noise, and let the dimensionality of the state $h$ be a free parameter. Therefore, the training function takes two input arguments: `noise_level` and `n_hidden_dim`.   You will later see how varying them affects reconstruction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import data_generator\n",
    "from __future__ import print_function\n",
    "\n",
    "#############################################################################\n",
    "# RNN model graph\n",
    "def build_rnn_regression_model(shape):\n",
    "    # shape is dict with keys:\n",
    "    # n_steps_per_batch, n_hidden_dim, n_input_dim\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # inputs to the dynamical system\n",
    "        X = tf.placeholder(tf.float32,\n",
    "                           [None, shape['n_steps_per_batch'], shape['n_input_dim']])\n",
    "        # observed state from the dynamical system\n",
    "        y = tf.placeholder(tf.float32, [None, shape['n_hidden_dim']])\n",
    "        \n",
    "        with tf.variable_scope('weights'):\n",
    "            # weight matrix\n",
    "            w = tf.get_variable('w', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            # initial state\n",
    "            h_0 = tf.get_variable('h_0', [shape['n_hidden_dim']])\n",
    "            \n",
    "        # for t = 1 to T, update state \n",
    "        h_t = h_0\n",
    "        for t in range(shape['n_steps_per_batch']):\n",
    "            x_t = X[:, t, :] #x_t.shape = [Batch_size,n_input]\n",
    "            h_t = tf.maximum(0.0, 1 - (tf.matmul(x_t, w) + h_t))\n",
    "        \n",
    "        # loss: L2\n",
    "        loss = tf.nn.l2_loss(h_t - y, name='loss') #L2 loss is mean square error loss\n",
    "        train_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "        summ = tf.summary.scalar('loss_sum_%dd' % shape['n_hidden_dim'], loss)# tf.summary.scalar(name,tensor), name='loss_sum_(num of hidden unit)'\n",
    "        \n",
    "    return {'inputs': [X, y], 'loss': loss, 'train_op': train_op, 'summ': summ,\n",
    "            'weights': {'w': w, 'h_0': h_0}, 'graph': g}\n",
    "\n",
    "#############################################################################\n",
    "# Main train loop for an RNN regression model\n",
    "# \n",
    "# This takes synthetic data generated by data_generator.build_dataset()\n",
    "# the weight matrix W is then inferred with back-prop \n",
    "def train_rnn_with_noise(noise_level, n_hidden_dim):\n",
    "    \n",
    "    # generate data\n",
    "    shapes = dict(n_hidden_dim=n_hidden_dim, n_input_dim=15, n_steps_per_batch=100)\n",
    "    rnn_dataset = data_generator.build_dataset('rnn', noise=noise_level, **shapes)# **shapes(or **anything): add a dict to dunction instad of feeding each argument for a function \n",
    "    (h0_true, w_true), batched_data = rnn_dataset  # \"true\" weights\n",
    "    # batched_data = data \n",
    "    \n",
    "    # build RNN model\n",
    "    model = build_rnn_regression_model(shapes)\n",
    "    \n",
    "    #logdir = './tensorboard/rnn_demo'  # if on Windows\n",
    "    #logdir = '/tmp/tensorboard/rnn_demo'  # if on Unix\n",
    "    logdir = 'summary_q1'\n",
    "    try:\n",
    "        os.makedirs(logdir)\n",
    "    except os.error:\n",
    "        pass\n",
    "    # If you want to see the plots, run tensorboard:\n",
    "    # $ tensorboard --logdir=[your_logdir]\n",
    "    #\n",
    "    # If you use SCC, you can forward the 6006 port from the cluster \n",
    "    # to your local machine via:\n",
    "    # $ ssh [SCC_cluster_name] -L 6006:localhost:6006\n",
    "    time_now = datetime.datetime.now().strftime(\"%d-%b-%H-%M-%S\")\n",
    "    run_name = 'hidden=%d_noise=%.2f' % (n_hidden_dim, noise_level)\n",
    "    sum_path = os.path.join(logdir, run_name + '_' + time_now)\n",
    "    print(sum_path)\n",
    "    max_iter_i = 0\n",
    "    with model['graph'].as_default() as g, tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sum_writer = tf.summary.FileWriter(sum_path, g)\n",
    "        for epoch_i in range(50):\n",
    "            loss_val, w_dist, h0_dist, iter_i = None, None, None, None\n",
    "            for iter_i, data_batch in enumerate(batched_data):#iter_i = n_batch, data_batch = each batch from n batches\n",
    "                max_iter_i = max(iter_i, max_iter_i)\n",
    "                global_step = epoch_i*max_iter_i+iter_i\n",
    "                \n",
    "                # run training step\n",
    "                train_feed_dict = dict(zip(model['inputs'], data_batch))\n",
    "                to_compute = [model['train_op'], model['summ'], model['loss'],\n",
    "                              model['weights']['w'], model['weights']['h_0']]\n",
    "                _, summ, loss_val, w_val, h0_val = sess.run(to_compute, train_feed_dict)\n",
    "                \n",
    "                # compute reconstruction error\n",
    "                w_err = np.linalg.norm(w_true-w_val)#L2 norm\n",
    "                h0_err = np.linalg.norm(h0_true-h0_val)\n",
    "                \n",
    "                # for tensorboard\n",
    "                sum_writer.add_summary(summ, global_step) #add_summary(summary or merged_summary , steps)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"w_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=w_err),\n",
    "                ]), global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"h_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=h0_err),\n",
    "                ]), global_step)\n",
    "                sum_writer.flush()#Call this method to make sure that all pending events have been written to disk\n",
    "                \n",
    "            print('epoch %d, loss=%g, w_err=%g'%(epoch_i, loss_val, w_err))\n",
    "            if global_step > 200: \n",
    "                break  # just train for 200 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.\n",
    "Now test the RNN wth varying noise levels and hidden dimensionalities.\n",
    "- For each combination of `n_hidden_dim` and `noise_level`, report the reconstruction error (`w_err`).\n",
    "- Describe how the hidden dimentionality and the noise level influence reconstruction quality. And briefly explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_hidden_dim 10, noise_level 0\n",
      "summary_q1/hidden=10_noise=0.00_24-Apr-20-35-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:97: DeprecationWarning: PyUnicode_AsEncodedObject() is deprecated; use PyUnicode_AsEncodedString() to encode from str to bytes or PyCodec_Encode() for generic encoding\n",
      "/home/drew/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:101: DeprecationWarning: PyUnicode_AsEncodedObject() is deprecated; use PyUnicode_AsEncodedString() to encode from str to bytes or PyCodec_Encode() for generic encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss=21.7464, w_err=2.27913\n",
      "epoch 1, loss=3.99693, w_err=0.821635\n",
      "epoch 2, loss=0.354323, w_err=0.249358\n",
      "epoch 3, loss=0.0353669, w_err=0.0978636\n",
      "epoch 4, loss=0.00661691, w_err=0.0500684\n",
      "epoch 5, loss=0.00157082, w_err=0.027675\n",
      "epoch 6, loss=0.000779871, w_err=0.0164796\n",
      "epoch 7, loss=0.000385589, w_err=0.0118495\n",
      "n_hidden_dim 10, noise_level 0.1\n",
      "summary_q1/hidden=10_noise=0.10_24-Apr-20-35-21\n",
      "epoch 0, loss=24.9035, w_err=1.96231\n",
      "epoch 1, loss=4.53444, w_err=0.94774\n",
      "epoch 2, loss=3.71061, w_err=0.591558\n",
      "epoch 3, loss=3.01951, w_err=0.586475\n",
      "epoch 4, loss=3.31639, w_err=0.614331\n",
      "epoch 5, loss=3.51989, w_err=0.650146\n",
      "epoch 6, loss=3.78551, w_err=0.684975\n",
      "epoch 7, loss=4.2381, w_err=0.748548\n",
      "n_hidden_dim 10, noise_level 0.5\n",
      "summary_q1/hidden=10_noise=0.50_24-Apr-20-35-27\n",
      "epoch 0, loss=62.5404, w_err=3.47266\n",
      "epoch 1, loss=36.4709, w_err=2.40816\n",
      "epoch 2, loss=39.8779, w_err=2.45214\n",
      "epoch 3, loss=42.4082, w_err=2.37396\n",
      "epoch 4, loss=43.6137, w_err=2.40301\n",
      "epoch 5, loss=43.7377, w_err=2.43722\n",
      "epoch 6, loss=44.314, w_err=2.45307\n",
      "epoch 7, loss=47.7056, w_err=2.48808\n",
      "n_hidden_dim 100, noise_level 0\n",
      "summary_q1/hidden=100_noise=0.00_24-Apr-20-35-33\n",
      "epoch 0, loss=350.201, w_err=7.88732\n",
      "epoch 1, loss=44.1284, w_err=2.67996\n",
      "epoch 2, loss=4.81999, w_err=0.925964\n",
      "epoch 3, loss=0.779983, w_err=0.33048\n",
      "epoch 4, loss=0.225985, w_err=0.173788\n",
      "epoch 5, loss=0.103811, w_err=0.129686\n",
      "epoch 6, loss=0.0694452, w_err=0.113319\n",
      "epoch 7, loss=0.0596605, w_err=0.0910307\n",
      "n_hidden_dim 100, noise_level 0.1\n",
      "summary_q1/hidden=100_noise=0.10_24-Apr-20-35-39\n",
      "epoch 0, loss=422.099, w_err=8.44293\n",
      "epoch 1, loss=61.7343, w_err=2.93642\n",
      "epoch 2, loss=25.1735, w_err=1.70528\n",
      "epoch 3, loss=22.8428, w_err=1.42686\n",
      "epoch 4, loss=24.8269, w_err=1.52752\n",
      "epoch 5, loss=27.8052, w_err=1.65413\n",
      "epoch 6, loss=30.203, w_err=1.79166\n",
      "epoch 7, loss=32.9364, w_err=1.90559\n",
      "n_hidden_dim 100, noise_level 0.5\n",
      "summary_q1/hidden=100_noise=0.50_24-Apr-20-35-45\n",
      "epoch 0, loss=924.406, w_err=10.6497\n",
      "epoch 1, loss=491.533, w_err=5.90024\n",
      "epoch 2, loss=471.792, w_err=5.98482\n",
      "epoch 3, loss=471.986, w_err=6.04528\n",
      "epoch 4, loss=483.272, w_err=6.18571\n",
      "epoch 5, loss=490.316, w_err=6.28728\n",
      "epoch 6, loss=500.736, w_err=6.33993\n",
      "epoch 7, loss=507.663, w_err=6.40752\n",
      "n_hidden_dim 1000, noise_level 0\n",
      "summary_q1/hidden=1000_noise=0.00_24-Apr-20-35-56\n",
      "epoch 0, loss=5594.21, w_err=31.2435\n",
      "epoch 1, loss=488.613, w_err=8.73933\n",
      "epoch 2, loss=55.574, w_err=2.90065\n",
      "epoch 3, loss=7.36841, w_err=1.064\n",
      "epoch 4, loss=1.53986, w_err=0.48702\n",
      "epoch 5, loss=0.83815, w_err=0.359273\n",
      "epoch 6, loss=0.780587, w_err=0.368487\n",
      "epoch 7, loss=0.56068, w_err=0.315282\n",
      "n_hidden_dim 1000, noise_level 0.1\n",
      "summary_q1/hidden=1000_noise=0.10_24-Apr-20-36-06\n",
      "epoch 0, loss=6158.82, w_err=33.2532\n",
      "epoch 1, loss=617.324, w_err=9.52914\n",
      "epoch 2, loss=244.695, w_err=5.25944\n",
      "epoch 3, loss=213.839, w_err=4.44147\n",
      "epoch 4, loss=229.45, w_err=4.58597\n",
      "epoch 5, loss=251.01, w_err=4.83623\n",
      "epoch 6, loss=274.267, w_err=5.13106\n",
      "epoch 7, loss=299.292, w_err=5.45291\n",
      "n_hidden_dim 1000, noise_level 0.5\n",
      "summary_q1/hidden=1000_noise=0.50_24-Apr-20-36-14\n",
      "epoch 0, loss=8831.35, w_err=37.3271\n",
      "epoch 1, loss=3891.61, w_err=18.8545\n",
      "epoch 2, loss=3820.42, w_err=18.5139\n",
      "epoch 3, loss=3919.84, w_err=18.5494\n",
      "epoch 4, loss=4043.93, w_err=18.895\n",
      "epoch 5, loss=4140.4, w_err=19.1395\n",
      "epoch 6, loss=4246.57, w_err=19.3589\n",
      "epoch 7, loss=4319.19, w_err=19.5273\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with different data noise levels and hidden dimentionalities\n",
    "# We are lucky to know the true hidden dimentionality in our simultaion\n",
    "for n_hidden_dim in [10, 100, 1000]:\n",
    "    for noise_level in [0, 0.1, 0.5]:\n",
    "        print('n_hidden_dim %d, noise_level %g' %(n_hidden_dim, noise_level))\n",
    "        train_rnn_with_noise(noise_level, n_hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer_Q1 :** \n",
    "\n",
    "\n",
    "|n_idden_dim| noise_level|w_err|\n",
    "|--|--|-----------------------------|\n",
    "| 10 | 0 |0.01185 |\n",
    "| 10 | 0.1 |0.748548 |\n",
    "| 10 | 0.5 |2.48808|\n",
    "| 100 | 0 |0.0910307 |\n",
    "| 100 | 0.1 |1.90559 |\n",
    "| 100 | 0.5 |6.40752 |\n",
    "| 1000 | 0 |0.315282|\n",
    "| 1000 | 0.1 |5.45291 |\n",
    "| 1000 | 0.5 |19.5273|\n",
    "\n",
    "It is obvoius that when we add noise, the error would be increased. So as we see the results, when we add noise and increse its level we get higher error.\n",
    "And also, from the table, when we increse number of hidden dim, the error is going to increase because when we number of hidden units are more then the size of matrix W increases so we have more elements to compute loss so our error increases as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: LSTM implementation\n",
    "\n",
    "(40 points)\n",
    "\n",
    "Now let's attempt to recover the weights in dynamical system simulated with an LSTM.  Although LSTMs are already implemented in TensorFlow ([tutorial here](https://www.tensorflow.org/tutorials/recurrent)) ([source here](https://github.com/tensorflow/tensorflow/blob/efe5376f3dec8fcc2bf3299a4ff4df6ad3591c88/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py#L264)), you will be implementing a simple LSTM from scratch using Tensorflow in this part. \n",
    "\n",
    "\n",
    "### Q2. Implement an LSTM \n",
    "Implement an LSTM model in an analogous way,  in functions `build_lstm_regression_model()` and `train_lstm_with_noise()` below.\n",
    "The RNN regression implementation above should give you some ideas.\n",
    "We already provided an LSTM version of the dynamical system generator in the dataset generator code.\n",
    "\n",
    "- Specifically, you can follow and implement Eq. 1-6 from [this link](http://deeplearning.net/tutorial/lstm.html) in `build_lstm_regression_model()`. \n",
    "You may simplify your code by concatenating $x$ and $h$.\n",
    "- Afterwards, implement `train_lstm_with_noise()` to train the LSTM and recover the parameters. Compute the reconstruction errors for $W_c$ and $U_c$, which are the parameters used in Eq. 2 in the link.\n",
    "- For each combination of hidden dimension and noise level, report the reconstruction error (`w_err`, `u_err`) you get from the LSTM.\n",
    "\n",
    "Note:\n",
    "- The weights might not get reconstructed correctly in the LSTM case even without noise. (Why?)\n",
    "Nevertheless, the loss should decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_lstm_regression_model(shape):\n",
    "    # shape is dict with keys:\n",
    "    # n_steps_per_batch, n_hidden_dim, n_input_dim\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # inputs\n",
    "        X = tf.placeholder(tf.float32,\n",
    "                           [None, shape['n_steps_per_batch'], shape['n_input_dim']])\n",
    "        # observed outputs\n",
    "        y = tf.placeholder(tf.float32, [None, shape['n_hidden_dim']])\n",
    "        \n",
    "        ####################  PUT YOUR CODE HERE   ######################\n",
    "      \n",
    "        with tf.variable_scope('weights'):\n",
    "            \n",
    "            w_i = tf.get_variable('W_i', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            w_f = tf.get_variable('W_f', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            w_c = tf.get_variable('W_c', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            w_o = tf.get_variable('W_o', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            \n",
    "            u_i = tf.get_variable('U_i', [shape['n_hidden_dim'] , shape['n_hidden_dim']])\n",
    "            u_f = tf.get_variable('U_f', [shape['n_hidden_dim'] , shape['n_hidden_dim']])\n",
    "            u_c = tf.get_variable('U_c', [shape['n_hidden_dim'] , shape['n_hidden_dim']])\n",
    "            u_o = tf.get_variable('U_o', [shape['n_hidden_dim'] , shape['n_hidden_dim']])\n",
    "            \n",
    "            v_o = tf.get_variable('V_o', [shape['n_hidden_dim'] , shape['n_hidden_dim']])\n",
    "            \n",
    "            h_0 = tf.get_variable('h_0', [1,shape['n_hidden_dim']])\n",
    "            c_0 = tf.get_variable('c_0' , [1,shape['n_hidden_dim']])\n",
    "            \n",
    "            c_t = c_0\n",
    "            h_t = h_0\n",
    "            for t in range(shape['n_steps_per_batch']):\n",
    "                x_t = X[:,t,:]\n",
    "                i_t = tf.nn.sigmoid(tf.matmul(x_t,w_i) + tf.matmul (h_t,u_i))\n",
    "                c_bar_t = tf.nn.tanh(tf.matmul(x_t,w_c) + tf.matmul (h_t,u_c))\n",
    "                f_t = tf.nn.sigmoid(tf.matmul(x_t,w_f) + tf.matmul (h_t,u_f))\n",
    "                c_t = tf.multiply(i_t,c_bar_t) + tf.multiply(f_t,c_t)\n",
    "                o_t = tf.nn.sigmoid(tf.matmul(x_t,w_o) + tf.matmul (h_t,u_o) + tf.matmul(c_t,v_o))\n",
    "                h_t = tf.multiply(o_t,tf.tanh(c_t))   \n",
    "            \n",
    "        \n",
    "        output = h_t  # put your result in variable 'output'\n",
    "        \n",
    "        #################################################################\n",
    "        # loss and train_op\n",
    "        loss = tf.nn.l2_loss(output - y, name='loss')\n",
    "        train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "        summ = tf.summary.scalar('loss_sum_%dd' % shape['n_hidden_dim'], loss)\n",
    "\n",
    "    return {'inputs': [X, y], 'loss': loss, 'train_op': train_op, 'summ': summ,\n",
    "            'weights': {'w_c': w_c, 'u_c': u_c}, \n",
    "            'graph': g} # we just return w_c and u_c cause they are \"true weights\" to recover\n",
    "    \n",
    "    \n",
    "def train_lstm_with_noise(noise_level, n_hidden_dim):\n",
    "    # generate data and random weights\n",
    "    shapes = dict(n_hidden_dim=n_hidden_dim, n_input_dim=15, n_steps_per_batch=100)\n",
    "    weights, batched_data = data_generator.build_dataset('lstm', noise=noise_level, **shapes)\n",
    "    w_c, u_c = weights[3], weights[7]  # the \"true\" weights to recover: Wc & Uc (in Eq.2)\n",
    "    \n",
    "    # this is the function you implemented\n",
    "    model = build_lstm_regression_model(shapes)\n",
    "    \n",
    "    #logdir = './tensorboard/lstm_demo'  # if on Windows\n",
    "    logdir = '/tmp/tensorboard/lstm_demo'  # if on Unix\n",
    "    try:\n",
    "        os.makedirs(logdir)\n",
    "    except os.error:\n",
    "        pass\n",
    "    time_now = datetime.datetime.now().strftime(\"%d-%b-%H-%M-%S\")\n",
    "    run_name = 'hidden=%d_noise=%.2f' % (n_hidden_dim, noise_level)\n",
    "    sum_path = os.path.join(logdir, run_name + '_' + time_now)\n",
    "    print(sum_path)\n",
    "    \n",
    "    max_iter_i = 0\n",
    "    with model['graph'].as_default() as g, tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sum_writer = tf.summary.FileWriter(sum_path, g)\n",
    "        for epoch_i in range(25):  # **********\n",
    "            loss_val, w_err, u_err, iter_i = None, None, None, None\n",
    "            for iter_i, data_batch in enumerate(batched_data):\n",
    "                max_iter_i = max(iter_i, max_iter_i)\n",
    "                global_step = epoch_i*max_iter_i+iter_i\n",
    "                \n",
    "                ###############################################################\n",
    "                ###################   PUT YOUR CODE HERE   ####################\n",
    "                train_feed_dict = dict(zip(model['inputs'],data_batch))\n",
    "                to_compute = [model['train_op'], model['summ'], model['loss'],\n",
    "                              model['weights']['w_c'], model['weights']['u_c']]\n",
    "                _, summ, loss_val, w_c_val, u_c_val = sess.run(to_compute, train_feed_dict)\n",
    "                w_err = np.linalg.norm(w_c - w_c_val)  # compute them and report\n",
    "                u_err = np.linalg.norm(u_c - u_c_val)\n",
    "                sum_writer.add_summary(summ, global_step) #add_summary(summary or merged_summary , steps)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"w_true_%dd\" % n_hidden_dim,\n",
    "                                      simple_value=w_err), #**** Look the rnn\n",
    "                ]), global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"u_true_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=u_err),\n",
    "                ]), global_step)\n",
    "                sum_writer.flush()#Call this method to make sure that all pending events have been written to disk\n",
    "                \n",
    "            print('epoch %d, loss=%g, w_err=%g, u_err=%g'%(epoch_i, loss_val, w_err, u_err))\n",
    "            #if global_step > 300: \n",
    "                #break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0\n",
      "/tmp/tensorboard/lstm_demo/hidden=10_noise=0.00_24-Apr-20-37-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:96: DeprecationWarning: PyUnicode_AsEncodedObject() is deprecated; use PyUnicode_AsEncodedString() to encode from str to bytes or PyCodec_Encode() for generic encoding\n",
      "/home/drew/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:100: DeprecationWarning: PyUnicode_AsEncodedObject() is deprecated; use PyUnicode_AsEncodedString() to encode from str to bytes or PyCodec_Encode() for generic encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss=19.415, w_err=13.7813, u_err=10.8849\n",
      "epoch 1, loss=18.6685, w_err=13.6967, u_err=10.889\n",
      "epoch 2, loss=18.0439, w_err=13.6129, u_err=10.8935\n",
      "epoch 3, loss=17.5041, w_err=13.5301, u_err=10.8977\n",
      "epoch 4, loss=17.0241, w_err=13.4485, u_err=10.9012\n",
      "epoch 5, loss=16.5875, w_err=13.3683, u_err=10.9038\n",
      "epoch 6, loss=16.1836, w_err=13.2897, u_err=10.9055\n",
      "epoch 7, loss=15.805, w_err=13.2129, u_err=10.906\n",
      "epoch 8, loss=15.4464, w_err=13.1378, u_err=10.9054\n",
      "epoch 9, loss=15.1039, w_err=13.0646, u_err=10.9037\n",
      "epoch 10, loss=14.7749, w_err=12.9932, u_err=10.9008\n",
      "epoch 11, loss=14.4576, w_err=12.9236, u_err=10.8967\n",
      "epoch 12, loss=14.1509, w_err=12.8558, u_err=10.8912\n",
      "epoch 13, loss=13.8536, w_err=12.7898, u_err=10.8843\n",
      "epoch 14, loss=13.5654, w_err=12.7256, u_err=10.8761\n",
      "epoch 15, loss=13.2858, w_err=12.6631, u_err=10.8663\n",
      "epoch 16, loss=13.0149, w_err=12.6025, u_err=10.8552\n",
      "epoch 17, loss=12.7527, w_err=12.5436, u_err=10.8427\n",
      "epoch 18, loss=12.4998, w_err=12.4865, u_err=10.8289\n",
      "epoch 19, loss=12.2565, w_err=12.4312, u_err=10.814\n",
      "epoch 20, loss=12.0232, w_err=12.3776, u_err=10.7981\n",
      "epoch 21, loss=11.8003, w_err=12.3256, u_err=10.7815\n",
      "epoch 22, loss=11.5878, w_err=12.2752, u_err=10.7642\n",
      "epoch 23, loss=11.3856, w_err=12.2262, u_err=10.7466\n",
      "epoch 24, loss=11.1936, w_err=12.1786, u_err=10.7286\n",
      "10 0.1\n",
      "/tmp/tensorboard/lstm_demo/hidden=10_noise=0.10_24-Apr-20-37-59\n",
      "epoch 0, loss=16.0665, w_err=11.5212, u_err=10.5928\n",
      "epoch 1, loss=15.4967, w_err=11.4227, u_err=10.5791\n",
      "epoch 2, loss=14.9634, w_err=11.326, u_err=10.5666\n",
      "epoch 3, loss=14.465, w_err=11.2312, u_err=10.5551\n",
      "epoch 4, loss=13.9999, w_err=11.1382, u_err=10.5446\n",
      "epoch 5, loss=13.5666, w_err=11.0472, u_err=10.535\n",
      "epoch 6, loss=13.1631, w_err=10.958, u_err=10.5259\n",
      "epoch 7, loss=12.7871, w_err=10.8707, u_err=10.5174\n",
      "epoch 8, loss=12.4361, w_err=10.7851, u_err=10.5094\n",
      "epoch 9, loss=12.1074, w_err=10.7014, u_err=10.5016\n",
      "epoch 10, loss=11.7981, w_err=10.6193, u_err=10.4941\n",
      "epoch 11, loss=11.5055, w_err=10.539, u_err=10.4869\n",
      "epoch 12, loss=11.2265, w_err=10.4604, u_err=10.48\n",
      "epoch 13, loss=10.9579, w_err=10.3835, u_err=10.4735\n",
      "epoch 14, loss=10.6963, w_err=10.3083, u_err=10.4674\n",
      "epoch 15, loss=10.438, w_err=10.2348, u_err=10.4619\n",
      "epoch 16, loss=10.1796, w_err=10.1629, u_err=10.457\n",
      "epoch 17, loss=9.91907, w_err=10.0927, u_err=10.4528\n",
      "epoch 18, loss=9.65719, w_err=10.024, u_err=10.4494\n",
      "epoch 19, loss=9.39814, w_err=9.95706, u_err=10.4467\n",
      "epoch 20, loss=9.14782, w_err=9.8918, u_err=10.4448\n",
      "epoch 21, loss=8.91109, w_err=9.82832, u_err=10.4435\n",
      "epoch 22, loss=8.69008, w_err=9.76665, u_err=10.4429\n",
      "epoch 23, loss=8.48446, w_err=9.70676, u_err=10.4428\n",
      "epoch 24, loss=8.29264, w_err=9.64862, u_err=10.4432\n",
      "10 0.5\n",
      "/tmp/tensorboard/lstm_demo/hidden=10_noise=0.50_24-Apr-20-38-39\n",
      "epoch 0, loss=16.794, w_err=11.5504, u_err=11.806\n",
      "epoch 1, loss=16.1148, w_err=11.4636, u_err=11.8006\n",
      "epoch 2, loss=15.5416, w_err=11.3787, u_err=11.7954\n",
      "epoch 3, loss=15.0476, w_err=11.2961, u_err=11.791\n",
      "epoch 4, loss=14.6155, w_err=11.216, u_err=11.7874\n",
      "epoch 5, loss=14.2338, w_err=11.1384, u_err=11.7846\n",
      "epoch 6, loss=13.8937, w_err=11.0633, u_err=11.7828\n",
      "epoch 7, loss=13.588, w_err=10.9907, u_err=11.7818\n",
      "epoch 8, loss=13.3111, w_err=10.9206, u_err=11.7816\n",
      "epoch 9, loss=13.0582, w_err=10.8526, u_err=11.7822\n",
      "epoch 10, loss=12.8254, w_err=10.7867, u_err=11.7833\n",
      "epoch 11, loss=12.6092, w_err=10.7226, u_err=11.785\n",
      "epoch 12, loss=12.4068, w_err=10.6601, u_err=11.7871\n",
      "epoch 13, loss=12.2159, w_err=10.5991, u_err=11.7896\n",
      "epoch 14, loss=12.0344, w_err=10.5392, u_err=11.7925\n",
      "epoch 15, loss=11.8605, w_err=10.4805, u_err=11.7956\n",
      "epoch 16, loss=11.6926, w_err=10.4226, u_err=11.799\n",
      "epoch 17, loss=11.5292, w_err=10.3655, u_err=11.8024\n",
      "epoch 18, loss=11.3689, w_err=10.3091, u_err=11.806\n",
      "epoch 19, loss=11.2101, w_err=10.2534, u_err=11.8096\n",
      "epoch 20, loss=11.0515, w_err=10.1982, u_err=11.8132\n",
      "epoch 21, loss=10.8917, w_err=10.1436, u_err=11.8166\n",
      "epoch 22, loss=10.7294, w_err=10.0897, u_err=11.8199\n",
      "epoch 23, loss=10.5634, w_err=10.0364, u_err=11.8229\n",
      "epoch 24, loss=10.3926, w_err=9.98396, u_err=11.8258\n",
      "100 0\n",
      "/tmp/tensorboard/lstm_demo/hidden=100_noise=0.00_24-Apr-20-39-28\n",
      "epoch 0, loss=160.832, w_err=40.3074, u_err=100.28\n",
      "epoch 1, loss=155.933, w_err=40.0534, u_err=100.275\n",
      "epoch 2, loss=151.842, w_err=39.8123, u_err=100.271\n",
      "epoch 3, loss=148.319, w_err=39.5848, u_err=100.267\n",
      "epoch 4, loss=145.21, w_err=39.3708, u_err=100.261\n",
      "epoch 5, loss=142.421, w_err=39.1697, u_err=100.255\n",
      "epoch 6, loss=139.879, w_err=38.9806, u_err=100.246\n",
      "epoch 7, loss=137.515, w_err=38.8021, u_err=100.236\n",
      "epoch 8, loss=135.271, w_err=38.6336, u_err=100.226\n",
      "epoch 9, loss=133.108, w_err=38.4747, u_err=100.216\n",
      "epoch 10, loss=130.998, w_err=38.3254, u_err=100.209\n",
      "epoch 11, loss=128.931, w_err=38.1859, u_err=100.203\n",
      "epoch 12, loss=126.908, w_err=38.056, u_err=100.201\n",
      "epoch 13, loss=124.935, w_err=37.9356, u_err=100.201\n",
      "epoch 14, loss=123.007, w_err=37.8242, u_err=100.205\n",
      "epoch 15, loss=121.107, w_err=37.7208, u_err=100.211\n",
      "epoch 16, loss=119.238, w_err=37.6251, u_err=100.219\n",
      "epoch 17, loss=117.46, w_err=37.5362, u_err=100.229\n",
      "epoch 18, loss=115.817, w_err=37.4535, u_err=100.239\n",
      "epoch 19, loss=114.224, w_err=37.3763, u_err=100.25\n",
      "epoch 20, loss=112.683, w_err=37.304, u_err=100.261\n",
      "epoch 21, loss=111.179, w_err=37.2342, u_err=100.271\n",
      "epoch 22, loss=109.774, w_err=37.167, u_err=100.278\n",
      "epoch 23, loss=108.302, w_err=37.0988, u_err=100.295\n",
      "epoch 24, loss=106.25, w_err=37.0384, u_err=100.307\n",
      "100 0.1\n",
      "/tmp/tensorboard/lstm_demo/hidden=100_noise=0.10_24-Apr-20-40-24\n",
      "epoch 0, loss=172.861, w_err=38.6068, u_err=100.318\n",
      "epoch 1, loss=166.697, w_err=38.3568, u_err=100.315\n",
      "epoch 2, loss=161.593, w_err=38.1201, u_err=100.313\n",
      "epoch 3, loss=157.188, w_err=37.8979, u_err=100.31\n",
      "epoch 4, loss=153.371, w_err=37.691, u_err=100.308\n",
      "epoch 5, loss=150.132, w_err=37.4994, u_err=100.306\n",
      "epoch 6, loss=147.305, w_err=37.3206, u_err=100.306\n",
      "epoch 7, loss=144.682, w_err=37.152, u_err=100.305\n",
      "epoch 8, loss=142.162, w_err=36.993, u_err=100.305\n",
      "epoch 9, loss=139.709, w_err=36.8433, u_err=100.305\n",
      "epoch 10, loss=137.331, w_err=36.703, u_err=100.307\n",
      "epoch 11, loss=135.031, w_err=36.5722, u_err=100.309\n",
      "epoch 12, loss=132.784, w_err=36.4507, u_err=100.311\n",
      "epoch 13, loss=130.56, w_err=36.3381, u_err=100.314\n",
      "epoch 14, loss=128.343, w_err=36.2344, u_err=100.318\n",
      "epoch 15, loss=126.133, w_err=36.1389, u_err=100.321\n",
      "epoch 16, loss=123.925, w_err=36.051, u_err=100.325\n",
      "epoch 17, loss=121.681, w_err=35.9699, u_err=100.33\n",
      "epoch 18, loss=119.416, w_err=35.8951, u_err=100.335\n",
      "epoch 19, loss=117.121, w_err=35.8251, u_err=100.344\n",
      "epoch 20, loss=115.107, w_err=35.7598, u_err=100.353\n",
      "epoch 21, loss=112.874, w_err=35.7021, u_err=100.364\n",
      "epoch 22, loss=110.864, w_err=35.6475, u_err=100.375\n",
      "epoch 23, loss=108.774, w_err=35.5963, u_err=100.386\n",
      "epoch 24, loss=106.97, w_err=35.5472, u_err=100.4\n",
      "100 0.5\n",
      "/tmp/tensorboard/lstm_demo/hidden=100_noise=0.50_24-Apr-20-41-17\n",
      "epoch 0, loss=171.776, w_err=38.5488, u_err=100.801\n",
      "epoch 1, loss=165.007, w_err=38.3348, u_err=100.801\n",
      "epoch 2, loss=160.094, w_err=38.1359, u_err=100.796\n",
      "epoch 3, loss=156.249, w_err=37.9523, u_err=100.787\n",
      "epoch 4, loss=153.091, w_err=37.7823, u_err=100.778\n",
      "epoch 5, loss=150.296, w_err=37.6224, u_err=100.769\n",
      "epoch 6, loss=147.652, w_err=37.4693, u_err=100.762\n",
      "epoch 7, loss=145.049, w_err=37.3213, u_err=100.758\n",
      "epoch 8, loss=142.454, w_err=37.1779, u_err=100.757\n",
      "epoch 9, loss=139.876, w_err=37.0394, u_err=100.758\n",
      "epoch 10, loss=137.34, w_err=36.907, u_err=100.761\n",
      "epoch 11, loss=134.869, w_err=36.7817, u_err=100.766\n",
      "epoch 12, loss=132.481, w_err=36.6639, u_err=100.773\n",
      "epoch 13, loss=130.184, w_err=36.554, u_err=100.781\n",
      "epoch 14, loss=127.982, w_err=36.4518, u_err=100.791\n",
      "epoch 15, loss=125.876, w_err=36.3571, u_err=100.802\n",
      "epoch 16, loss=123.862, w_err=36.2694, u_err=100.813\n",
      "epoch 17, loss=121.878, w_err=36.1885, u_err=100.824\n",
      "epoch 18, loss=119.853, w_err=36.1139, u_err=100.834\n",
      "epoch 19, loss=117.845, w_err=36.045, u_err=100.843\n",
      "epoch 20, loss=115.897, w_err=35.9808, u_err=100.852\n",
      "epoch 21, loss=113.975, w_err=35.921, u_err=100.86\n",
      "epoch 22, loss=112.073, w_err=35.8646, u_err=100.867\n",
      "epoch 23, loss=110.25, w_err=35.808, u_err=100.88\n",
      "epoch 24, loss=109.061, w_err=35.7513, u_err=100.878\n",
      "1000 0\n",
      "/tmp/tensorboard/lstm_demo/hidden=1000_noise=0.00_24-Apr-20-44-52\n",
      "epoch 0, loss=1879.67, w_err=123.625, u_err=1000.89\n",
      "epoch 1, loss=1830.67, w_err=123.351, u_err=1000.88\n",
      "epoch 2, loss=1769.94, w_err=123.114, u_err=1000.88\n",
      "epoch 3, loss=1696.37, w_err=122.909, u_err=1000.87\n",
      "epoch 4, loss=1640.51, w_err=122.737, u_err=1000.86\n",
      "epoch 5, loss=1580.64, w_err=122.595, u_err=1000.84\n",
      "epoch 6, loss=1487.69, w_err=122.464, u_err=1000.82\n",
      "epoch 7, loss=1407.21, w_err=122.36, u_err=1000.81\n",
      "epoch 8, loss=1311.79, w_err=122.259, u_err=1000.78\n",
      "epoch 9, loss=1228.46, w_err=122.179, u_err=1000.76\n",
      "epoch 10, loss=1122.85, w_err=122.102, u_err=1000.73\n",
      "epoch 11, loss=1054.49, w_err=122.028, u_err=1000.71\n",
      "epoch 12, loss=976.635, w_err=121.963, u_err=1000.69\n",
      "epoch 13, loss=890.676, w_err=121.912, u_err=1000.67\n",
      "epoch 14, loss=829.424, w_err=121.858, u_err=1000.65\n",
      "epoch 15, loss=769.015, w_err=121.811, u_err=1000.64\n",
      "epoch 16, loss=709.279, w_err=121.761, u_err=1000.62\n",
      "epoch 17, loss=660.49, w_err=121.724, u_err=1000.61\n",
      "epoch 18, loss=620.017, w_err=121.688, u_err=1000.59\n",
      "epoch 19, loss=607.68, w_err=121.655, u_err=1000.58\n",
      "epoch 20, loss=550.467, w_err=121.623, u_err=1000.57\n",
      "epoch 21, loss=517.226, w_err=121.599, u_err=1000.56\n",
      "epoch 22, loss=487.966, w_err=121.57, u_err=1000.55\n",
      "epoch 23, loss=461.446, w_err=121.547, u_err=1000.53\n",
      "epoch 24, loss=429.553, w_err=121.528, u_err=1000.54\n",
      "1000 0.1\n",
      "/tmp/tensorboard/lstm_demo/hidden=1000_noise=0.10_24-Apr-20-51-34\n",
      "epoch 0, loss=1909.27, w_err=123.295, u_err=999.847\n",
      "epoch 1, loss=1863.17, w_err=123.021, u_err=999.85\n",
      "epoch 2, loss=1803.94, w_err=122.782, u_err=999.852\n",
      "epoch 3, loss=1722.3, w_err=122.578, u_err=999.852\n",
      "epoch 4, loss=1651.34, w_err=122.409, u_err=999.84\n",
      "epoch 5, loss=1578.89, w_err=122.262, u_err=999.822\n",
      "epoch 6, loss=1507.26, w_err=122.139, u_err=999.797\n",
      "epoch 7, loss=1423.04, w_err=122.025, u_err=999.779\n",
      "epoch 8, loss=1314.09, w_err=121.931, u_err=999.754\n",
      "epoch 9, loss=1221.8, w_err=121.849, u_err=999.729\n",
      "epoch 10, loss=1137.13, w_err=121.769, u_err=999.702\n",
      "epoch 11, loss=1040.31, w_err=121.698, u_err=999.684\n",
      "epoch 12, loss=965.027, w_err=121.629, u_err=999.664\n",
      "epoch 13, loss=894.985, w_err=121.569, u_err=999.647\n",
      "epoch 14, loss=810.915, w_err=121.518, u_err=999.634\n",
      "epoch 15, loss=761.388, w_err=121.469, u_err=999.622\n",
      "epoch 16, loss=725.504, w_err=121.423, u_err=999.601\n",
      "epoch 17, loss=664.728, w_err=121.382, u_err=999.591\n",
      "epoch 18, loss=619.601, w_err=121.342, u_err=999.585\n",
      "epoch 19, loss=586.73, w_err=121.312, u_err=999.575\n",
      "epoch 20, loss=539.361, w_err=121.279, u_err=999.559\n",
      "epoch 21, loss=518.49, w_err=121.25, u_err=999.554\n",
      "epoch 22, loss=481.856, w_err=121.224, u_err=999.544\n",
      "epoch 23, loss=488.047, w_err=121.201, u_err=999.55\n",
      "epoch 24, loss=433.475, w_err=121.181, u_err=999.543\n",
      "1000 0.5\n",
      "/tmp/tensorboard/lstm_demo/hidden=1000_noise=0.50_24-Apr-20-58-18\n",
      "epoch 0, loss=1888.35, w_err=123.341, u_err=999.641\n",
      "epoch 1, loss=1851.41, w_err=123.086, u_err=999.642\n",
      "epoch 2, loss=1807.07, w_err=122.85, u_err=999.65\n",
      "epoch 3, loss=1737.44, w_err=122.643, u_err=999.658\n",
      "epoch 4, loss=1675.57, w_err=122.479, u_err=999.649\n",
      "epoch 5, loss=1624.72, w_err=122.343, u_err=999.639\n",
      "epoch 6, loss=1523.33, w_err=122.214, u_err=999.616\n",
      "epoch 7, loss=1449.22, w_err=122.113, u_err=999.604\n",
      "epoch 8, loss=1358.94, w_err=122.015, u_err=999.586\n",
      "epoch 9, loss=1259.24, w_err=121.933, u_err=999.567\n",
      "epoch 10, loss=1159.24, w_err=121.854, u_err=999.54\n",
      "epoch 11, loss=1085.87, w_err=121.782, u_err=999.524\n",
      "epoch 12, loss=997.811, w_err=121.72, u_err=999.518\n",
      "epoch 13, loss=918.287, w_err=121.663, u_err=999.498\n",
      "epoch 14, loss=882.346, w_err=121.61, u_err=999.491\n",
      "epoch 15, loss=786.232, w_err=121.559, u_err=999.475\n",
      "epoch 16, loss=751.297, w_err=121.518, u_err=999.465\n",
      "epoch 17, loss=686.249, w_err=121.484, u_err=999.46\n",
      "epoch 18, loss=629.308, w_err=121.453, u_err=999.447\n",
      "epoch 19, loss=597.229, w_err=121.417, u_err=999.435\n",
      "epoch 20, loss=555.218, w_err=121.393, u_err=999.435\n",
      "epoch 21, loss=546.895, w_err=121.365, u_err=999.424\n",
      "epoch 22, loss=496.536, w_err=121.342, u_err=999.419\n",
      "epoch 23, loss=474.452, w_err=121.319, u_err=999.407\n",
      "epoch 24, loss=447.242, w_err=121.3, u_err=999.403\n"
     ]
    }
   ],
   "source": [
    "for n_hidden_dim in [10, 100, 1000]:\n",
    "    for noise_level in [0, 0.1, 0.5]:\n",
    "        print(n_hidden_dim, noise_level)\n",
    "        train_lstm_with_noise(noise_level, n_hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.Answer\n",
    "\n",
    "\n",
    "|n_idden_dim| noise_level|w_err|u_err\n",
    "|--|--|-----------------------------|\n",
    "| 10 | 0 |12.1786|10.7286|\n",
    "| 10 | 0.1 |9.64862|10.4432|\n",
    "| 10 | 0.5 |9.98396|11.8258|\n",
    "| 100 | 0 |37.0384 |100.307\n",
    "| 100 | 0.1 |35.5472|100.4|\n",
    "| 100 | 0.5 |35.7513 |100.878|\n",
    "| 1000 | 0 |121.528|1000.54|\n",
    "| 1000 | 0.1 |121.181 |999.543\n",
    "| 1000 | 0.5 |121.3|999.403|\n",
    "\n",
    "As you see from the table, even in the cases that we do not have any noise, loss is decreasing but errors increase. This is because in learning process, we just decrease loss (in gradient method) we do not do anything for decreasing weights. SO we can conclude that there exists another weights that can decrease after some iteration as well as decreasing loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-modal Restricted Boltzmann Machines\n",
    "\n",
    "(30 points)\n",
    "\n",
    "- Relevant reading: Goodfellow chapter 20 through 20.8, in particular 20.2 and 20.3. Also see the [lecture notes](https://drive.google.com/file/d/0B8xkaMshuaF9dFVRclo3RENKdUk/view?usp=sharing).\n",
    "\n",
    "For this question, we will consider designing an unsupervised probability distribution  over two modalities.  Suppose we have data consisting of images and their associated captions, so that each input data point may be thought of as the pair $(t,v)$, where $t = (t_1, ..., t_n)$ is the text data and $v = (v_1, ..., v_m)$ is the visual data.  For simplicity, we will consider both as vector inputs (so, for instance, we will not be considering any two-dimensional convolutions for the visual data); the main difference between the two inputs is that the text input is discrete-valued, where we assume that each $t_i$ can take on $k$ different values, and each visual input is real-valued.\n",
    "\n",
    "Following our discussion of Restricted Boltzmann Machines (RBMs), we will further consider a hidden layer with **binary** hidden units $(h_1, ..., h_d)$, and define a joint probability distribution over an input data point and a hidden layer.  In particular, let us define two matrices $W_t\\in\\mathbb{R}^{n\\times d}$ and $W_v\\in\\mathbb{R}^{m\\times d}$; these matrices correspond to weights between text input and hidden units, and weights between visual input and hidden units, respectively.  In addition, we have the bias vectors $a$, $b$, and $c$, associated with the text, visual, and hidden units, respectively.\n",
    "The multi-modal RBM is illustrated below.\n",
    "\n",
    "<img src=\"MultimodalRBM.png\" width=\"550\">\n",
    "\n",
    "Next we define the joint probability distribution $p(t,v,h)$ over text inputs, visual inputs, and hidden states.  First we let the energy function be: \n",
    "\n",
    "$E(v,t,h) = -t^T W_t h - v^T W_v h - a^T t + (0.5 t^Tt - b^T v + 0.5 v^T v) - c^T h$.\n",
    "\n",
    "Then let the joint probability be defined as:\n",
    "\n",
    "$p(v,t,h) = \\frac{1}{Z}exp(-E(v,t,h)),$\n",
    "\n",
    "where $Z$ is the normalizing constant that ensures that the probability distribution is properly normalized to sum/integrate to one.\n",
    "\n",
    "Our goal is to perform maximum likelihood estimation for the parameters $W_t$, $W_v$, and $c$ (we will assume $a$ and $b$ are fixed and known), given a set of $N$ data points $D = \\{(t^{(1)},v^{(1)}), ..., (t^{(N)},v^{(N)})\\}$.  As is typical in such settings, we assume that the data in $D$ are i.i.d. samples.\n",
    "\n",
    "Please write your solutions in the cells below, or hand in a hard copy.\n",
    "For each question you must show your work to receive full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.1 \n",
    "\n",
    "Write down the marginal probability for a single data point $(t^{(1)}, v^{(1)})$, namely $p(t^{(1)}, v^{(1)})$. Treating this probability as the likelihood for a single point, write down the log-likelihood for the entire data set $D$ as a function of the parameters $(W_t, W_v,  c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "$$p(t^{(1)}, v^{(1)}) = \\frac{1}{Z}exp(-E(v,t,h)) = \\sum_h p(t^{(1)}, v^{(1)},h) = $$\n",
    "$$\\sum_h \\frac{1}{Z}exp[{t^{(1)}}^T W_t h + {v^{(1)}}^T W_v h + a^T t^{(1)} - 0.5 {t^{(1)}}^Tt^{(1)} + b^Tv^{(1)} - 0.5 {v^{(1)}}^T v^{(1)} + c^T h]$$\n",
    "\n",
    "For all data: $$ \\mathcal L(W_t,W_v,c | D) = \\prod_{i}^{N} p(t^{(i)}, v^{(i)}) $$\n",
    "Log liklihood: $$ \\log (L(W_t,W_v,c|D)) = \\sum_{i}^{N} \\log (\\sum_h \\frac{1}{Z}exp[{t^{(i)}}^T W_t h + {v^{(i)}}^T W_v h + a^T t^{(i)} - 0.5 {t^{(i)}}^Tt^{(i)} + b^Tv^{(i)} - 0.5 {v^{(i)}}^T v^{(i)} + c^T h]) =$$\n",
    "$$ -N \\log(Z) + \\sum_{i}^{N}\\log (\\sum_h \\frac{1}{Z}exp[{t^{(i)}}^T W_t h + {v^{(i)}}^T W_v h + a^T t^{(i)} - 0.5 {t^{(i)}}^Tt^{(i)} + b^Tv^{(i)} - 0.5 {v^{(i)}}^T v^{(i)} + c^T h]) $$\n",
    "We know that : $$ Z = \\sum_t \\sum_v \\sum_h exp(-E(v,t,h)) \\rightarrow -N \\log Z = -N \\log(\\sum_{t,v,h}exp(-E(v,t,h))) $$\n",
    "So now we have: \n",
    "$$   \\log (L(W_t,W_v,c|D)) =-N \\log(\\sum_{t,v,h}exp(t^T W_t h + v^T W_v h + a^T t - (0.5 t^Tt - b^T v + 0.5 v^T v) + c^T h)) +  \n",
    "\\sum_{i}^{N}\\log {(\\sum_hexp[{t^{(i)}}^T W_t h + {v^{(i)}}^T W_v h + a^T t^{(i)} - 0.5 {t^{(i)}}^Tt^{(i)} + b^Tv^{(i)} + 0.5 {v^{(i)}}^T v^{(i)} - c^T h])} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.2 \n",
    "\n",
    "Write down the partial derivative of $E(t,v,h)$ with respect to each of the parameters, i.e., each entry of $W_t$, each entry of $W_v$, and each entry of $c$. You can also directly write in matrix/vector forms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "$$ \\frac{\\partial E(v,t,h)}{\\partial W_t }  = -t\\ h^{T} $$\n",
    "\n",
    "$$ \\frac{\\partial E(v,t,h)}{\\partial W_v} = -v\\ h^{T} $$\n",
    "\n",
    "$$ \\frac{\\partial E(v,t,h)}{\\partial c} = - h^{T}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.3 \n",
    "\n",
    "Derive the conditional distributions $p(h | v, t)$, $p(v | h)$ and $p(t | h)$, either in elementwise form or in vector form.\n",
    "Also establish that $v$ and $t$ are conditionally independent given $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "$$ p(h | v, t) = \\frac{p(h , v, t) }{p(v, t)}= \\frac{\\frac{1}{Z} exp(-E(h,v,t))}{p(v,t)} =\\frac{ \\frac{1}{Z}exp(t^T W_t h + v^T W_v h + a^T t - (0.5 t^Tt - b^T v + 0.5 v^T v) - c^T h)}{p(v,t)}=$$\n",
    "\n",
    "$$ \\frac{1}{Z'}exp(t^T W_t h + v^T W_v h + c^T h) =  \\frac{1}{Z'}exp(\\sum_{j=1}^{d}t^T W_{t_{:,j}} h_j + \\sum_{j=1}^{d}v^T W_{v_{:,j}} h_j + \\sum_{j=1}^{d}c_j h_j) = \\frac{1}{Z'} \\prod_{j=1}^{d}exp({t^T W_{t_{:,j}} h_j + v^T W_{v_{:,j}} h_j + c_j h_j}) $$\n",
    "\n",
    "So we can factorize conditional probability over $h_j$:\n",
    "$$ p(h_j=1 | v, t) = \\frac{exp(t^T W_{t_{:,j}} + v^T W_{v_{:,j}} + c_j)}{exp(0)+exp(t^T W_{t_{:,j}} + v^T W_{v_{:,j}} + c_j)}  = \\sigma(t^T W_{t_{:,j}} + v^T W_{v_{:,j}} + c_j)$$\n",
    "Now, we would have:\n",
    "$$ p(h |  v, t) = \\prod_{j=1}^{d} \\sigma\\big((2h-1)\\odot (t^T W_t + v^T W_v + c)\\big)_j $$\n",
    "\n",
    "For $p(v,t|h):$\n",
    "\n",
    "$$p(v,t | h) = \\frac{p(h , v, t) }{p(h)} = \\frac{\\frac{1}{Z} exp(-E(h,v,t))}{p(h)} = \\frac{ \\frac{1}{Z}exp(t^T W_t h + v^T W_v h + a^T t - (0.5 t^Tt - b^T v + 0.5 v^T v) - c^T h)}{p(v,t)}=$$\n",
    "\n",
    "$$ \\frac{1}{Z'}exp(a^T t - 0.5 t^Tt + b^T v - 0.5 v^T v) = \\frac{1}{Z'}exp(a^T t - 0.5 t^Tt)exp(b^T v - 0.5 v^T v) \\  \\propto p(t|h)p(v|h)$$\n",
    "\n",
    "Then we can factorize $p(v|h)$ and $p(t|h)$ as following: \n",
    " \n",
    "$$p(v_j|h) = \\frac{1}{Z'} \\prod_{j=1}^{m}exp(v_j W_{v_{:,j}} h + b_j v_j   -  0.5 v_j^2) =\\sigma((2v_j-1)(v_j W_{v_{:,j}} h + b_j v_j   -  0.5 v_j^2) ) $$\n",
    "Then $$ p(v | h) = \\prod_{j=1}^{m} \\sigma\\big((2v-1)\\odot (W_v h + b^T + 0.5v)\\big)_j  $$\n",
    "Similarly:\n",
    "$$ p(t_j|h) = \\sigma \\big((2t_j-1)(t_j W_{t_{:,j}} h + a_j t_j   -  0.5 t_j^2) \\big)  $$\n",
    "\n",
    "$$ p(t | h) = \\prod_{j=1}^{n} \\sigma\\big((2t-1)\\odot (W_t h + a^T + 0.5t)\\big)_j  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.4\n",
    "\n",
    "Write down the partial derivative of the log-likelihood for $D$ with respect to each of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "$$\\frac{\\partial L(W_t,W_v,c | D)}{\\partial W_t }  = -N \\frac{\\partial}{\\partial W_t } \\log(\\sum_{h,v,t} exp(-E(v,t,h))) + \\frac{\\partial}{\\partial W_t }\\sum_{i=1}^N \\ log \\sum_h exp(-E(v^{(i)},t^{(i)},h)) $$\n",
    "First term:\n",
    "\n",
    "$$ -N \\frac{\\partial}{\\partial W_t } \\log(\\sum_{h,v,t} exp(-E(v,t,h))) = \\frac{-N}{\\sum_{h,v,t} exp(-E(v,t,h))} \\frac{\\partial}{\\partial W_t }\\sum_{h,v,t} exp(-E(v,t,h)) =$$\n",
    "\n",
    "$$\\frac{N}{\\sum_{h,v,t} exp(-E(v,t,h))} \\sum_{h,v,t} exp(-E(v,t,h))\\frac{\\partial E(v,t,h)}{\\partial W_t } = \\frac{-N}{\\sum_{h,v,t} exp(-E(v,t,h))} \\sum_{h,v,t} exp(-E(v,t,h))\\ t\\  h^T  $$\n",
    "\n",
    "From the definiton of $p(t,v,h):$\n",
    "\n",
    "$$= -N  \\sum_{h,v,t} p(v,t,h)\\ t\\  h^T = -N\\  E_{Model}[th^T]$$\n",
    "\n",
    "The second term: \n",
    "\n",
    "$$\\frac{\\partial}{\\partial W_t }\\sum_{i=1}^N \\ log \\sum_h exp(-E(v^{(i)},t^{(i)},h)) = \\sum_{i=1}^N \\frac{\\partial}{\\partial W_t } \\log \\sum_h  exp(-E(v^{(i)},t^{(i)},h)) =  $$\n",
    "\n",
    "$$= \\sum_{i=1}^N \\frac{-1}{\\sum_h  exp(-E(v^{(i)},t^{(i)},h))}\\sum_h exp(-E(v^{(i)},t^{(i)},h))\\frac{\\partial}{\\partial W_t }E(v^{(i)},t^{(i)},h) =\\sum_{i=1}^N \\sum_h \\frac{exp(-E(v^{(i)},t^{(i)},h))}{\\sum_h  exp(-E(v^{(i)},t^{(i)},h))} t^{(i)} h^T$$\n",
    "\n",
    "$$= \\sum_{i=1}^N \\sum_h p(h|t^i,v^i) \\ t\\  h^T =  NE_{Data}[t h^T]$$ \n",
    "\n",
    "$$\\frac{\\partial L(W_t,W_v,c | D)}{\\partial W_t }  = N(E_{Data}[th^T]- E_{model}[th^T]) $$\n",
    "\n",
    "Similarly we have done the same approach for $\\frac {\\partial L(W_t,W_v,c | D)}{\\partial W_v }$:\n",
    "\n",
    "$$ \\frac{\\partial L(W_t,W_v,c | D)}{\\partial W_v }  = N (E_{Data}[vh^T] -E_{model}[vh^T]) $$\n",
    "\n",
    "And for $\\frac {\\partial L(W_t,W_v,c | D)}{\\partial c }$:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L(W_t,W_v,c | D) }{\\partial c }= -N \\frac{\\partial}{\\partial c } \\log(\\sum_{h,v,t} exp(-E(v,t,h))) + \\frac{\\partial}{\\partial c }\\sum_{i=1}^N \\ log \\sum_h exp(-E(v^{(i)},t^{(i)},h))$$\n",
    "\n",
    "For the first term: \n",
    "\n",
    "$$-N \\frac{\\partial}{\\partial c } \\log(\\sum_{h,v,t} exp(-E(v,t,h))) = \\frac{-N}{\\sum_{h,v,t} exp(-E(v,t,h))} \\frac{\\partial}{\\partial c }\\sum_{h,v,t} exp(-E(v,t,h)) =$$\n",
    "\n",
    "$$\\frac{N}{\\sum_{h,v,t} exp(-E(v,t,h))} \\sum_{h,v,t} exp(-E(v,t,h))\\frac{\\partial E(v,t,h)}{\\partial c } = \\frac{-N}{\\sum_{h,v,t} exp(-E(v,t,h))} \\sum_{h,v,t} exp(-E(v,t,h))h^T = $$\n",
    "\n",
    "$$ = -N  \\sum_{h,v,t} p(v,t,h)h^T = -NE_{Model}[h^T] $$\n",
    "\n",
    "For the second expression: \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial c }\\sum_{i=1}^N \\ log \\sum_h exp(-E(v^{(i)},t^{(i)},h)) = \\sum_{i=1}^N \\frac{\\partial}{\\partial c } \\log \\sum_h  exp(-E(v^{(i)},t^{(i)},h)) =  $$\n",
    "\n",
    "$$\\sum_{i=1}^{N} \\frac{-1}{\\sum_h exp(-E(v^{(i)},t^{(i)},h))} \\sum_h exp(-E(v^{(i)},t^{(i)},h))\\frac{\\partial}{\\partial c}E(v^{(i)},t^{(i)},h) = \\sum_{i=1}^{N} \\sum_h \\frac{exp(-E(v^{(i)},t^{(i)},h))}{\\sum_h exp(-E(v^{(i)},t^{(i)},h))} h^T= $$\n",
    "\n",
    "$$\\sum_{i=1}^N \\sum_h p(h|t^i,v^i) h^T =  NE_{Data}[h^T]$$ \n",
    "\n",
    "Then we are going to get:\n",
    "\n",
    "$$\\frac {\\partial L(W_t,W_v,c | D)}{\\partial c } = N(E_{Data}[h^T] - E_{Model}[h^T])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q4.5 \n",
    "\n",
    "Extend the contrastive divergence approach for standard RBMs, as discussed in class, to suggest a gradient ascent procedure for learning the parameters of our multi-modal RBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "For using gradient based optimization we are going to encounter with some issues. From the last result of previous part, first, we have to compute $NE_{Data}[t h^T]$ which is equal to $\\sum_{i=1}^N \\sum_h p(h|t^i,v^i) \\ t\\  h^T $ it means that we have to compute this value for $2^d$ different cases. And it does not seem computationally resasonable. So we just take sample from $p(h|t^i,v^i)$ I mean we samplize $p(h|t^i,v^i)$. \n",
    "For the second one, problem is more complicated, cause we have to compute joint probability distribution, $\\sum_{h,v,t} p(v,t,h)\\ t\\  h^T $ so we have a lot of cases. So here again we have to sample from joint distribution. As it mentioned in the class, first we cinsider a random data point from our data set like, $ v^{(i)} , t^{(i)}$ and we sample from $p(h|v,t)$which is be coputed in part3, and then we get the h and now we sample from $p(v,t|h)=p(v|h)p(t|h)$ and get new t and v and do this cycle again and agian. So we are going to get estimation of $E_{data} , E_{model}$ Now we can use gradient method. Gradient acseent would be like:\n",
    "\n",
    "$W_t(New) = W_t^t + \\lambda*  estimations$\n",
    "\n",
    "$W_v(New) = W_v^t + \\lambda* estimations$\n",
    "\n",
    "$c(New) = c^t + \\lambda* estimations$\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
