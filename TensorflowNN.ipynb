{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: MNIST Softmax Classifier Demo in TensorFlow\n",
    "\n",
    "TensorFlow is already installed on the SCC. Please review the instructions on [connecting to SCC](https://docs.google.com/document/d/1C4XrrTZIRfmJ6-LHmuJ4levTnunezujVrxCz8jJ4rec), and [running jobs on SCC](http://www.bu.edu/tech/support/research/system-usage/running-jobs/).\n",
    "\n",
    "Make sure you are capable of running this demo (using `qsub`) on the SCC cluster: [`mnist_softmax_scope.py`](https://github.com/kunhe/cs591s2/blob/master/mnist_softmax_scope.py). There is no required deliverable, but this exercise is helpful for running jobs on the SCC in the future.\n",
    "\n",
    "The demo is another implementation of `mnist_softmax.py` presented in the MNIST for ML Beginners tutorial, but using scopes. For the purposes of this assignment, the contents of the demo is also replicated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-1-90e2c1b09fc7>:53: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "iteration 0\t accuracy: 0.405\n",
      "iteration 2000\t accuracy: 0.918\n",
      "iteration 4000\t accuracy: 0.920\n",
      "iteration 6000\t accuracy: 0.922\n",
      "iteration 8000\t accuracy: 0.921\n",
      "iteration 10000\t accuracy: 0.925\n",
      "iteration 12000\t accuracy: 0.923\n",
      "iteration 14000\t accuracy: 0.923\n",
      "iteration 16000\t accuracy: 0.923\n",
      "iteration 18000\t accuracy: 0.924\n",
      "iteration 20000\t accuracy: 0.925\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def logistic_regression(x_):\n",
    "    # create the actual model\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=1e-4)}\n",
    "    with tf.variable_scope(\"weights\", **scope_args):\n",
    "        W = tf.get_variable('W', shape=[784, 10])\n",
    "        b = tf.get_variable('b', shape=[10])\n",
    "        y_logits = tf.matmul(x_, W) + b\n",
    "    return y_logits\n",
    "\n",
    "def test_classification(model_function, learning_rate=0.1):\n",
    "    # import data\n",
    "    mnist = input_data.read_data_sets('./datasets/mnist/', one_hot=True)\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, 784])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_logits)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(y_pred, tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        # train\n",
    "        for iter_i in range(20001):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 2000 == 0:\n",
    "                tf_feed_dict = {x_: mnist.test.images, y_: mnist.test.labels}\n",
    "                acc_value = sess.run(accuracy, feed_dict=tf_feed_dict)\n",
    "                print ('iteration %d\\t accuracy: %.3f'%(iter_i, acc_value))\n",
    "                \n",
    "test_classification(logistic_regression, learning_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building Neural Networks in TensorFlow\n",
    "\n",
    "(45 points)\n",
    "\n",
    "### Q2.1 MLP in TensorFlow\n",
    "\n",
    "Task: \n",
    "\n",
    "- Implement a multi-layer perceptron function **`mlp(x, hidden_sizes, activation_fn)`** in TensorFlow that has the following input and output: \n",
    "\n",
    " **Inputs**\n",
    " - `x`, an input tensor of the images in the current batch `[batch_size, 28x28]`\n",
    " - `hidden_sizes`, a list of the number of hidden units per layer. For example: `[5,2]` means  5 hidden units in the first layer, and 2 hidden units in the second (output) layer.\n",
    " (Note: for MNIST, we need `hidden_sizes[-1]==10` since it has 10 classes.)\n",
    " - `activation_fn`, the activation function to be applied\n",
    "\n",
    " **Output**\n",
    " - a tensor of shape `[batch_size, hidden_sizes[-1]]`. \n",
    "\n",
    "**Note: ** \n",
    "- Make sure the activation function is not applied to the final (output) layer.\n",
    "- It is recommended to use scopes and `tf.get_variable()` (as opposed to `tf.Variable()` which you may see elsewhere), as demonstrated in the sample code, and explained in the \"Sharing Variables\" tutorial in Part 0. Also see [here](http://stackoverflow.com/questions/37098546/difference-between-variable-and-get-variable-in-tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mlp(x, hidden_sizes, activation_fn):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=2e-1)}\n",
    "    with tf.variable_scope(\"weights\", **scope_args):\n",
    "        hidden_sizes = [784] + hidden_sizes   \n",
    "        h = x\n",
    "        for i in range(len(hidden_sizes)-1):           \n",
    "            with tf.variable_scope(\"hidden\" + str(i)) as scope:\n",
    "                temp = h\n",
    "                W = tf.get_variable('W', shape=[hidden_sizes[i], hidden_sizes[i+1]])\n",
    "                b = tf.get_variable('b', shape=hidden_sizes[i+1])\n",
    "                h = activation_fn(tf.matmul(h, W) + b) \n",
    "                \n",
    "        y_logits = tf.matmul(temp, W) + b\n",
    "                     \n",
    "    return y_logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code tests your  implementation of the **`mlp()`** function. It basically recreates the 2-layer MLP with ReLU activation that you implemented in Problem Set 2. It should give an accuracy of above 0.97."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-1-90e2c1b09fc7>:53: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "iteration 0\t accuracy: 0.123\n",
      "iteration 2000\t accuracy: 0.939\n",
      "iteration 4000\t accuracy: 0.955\n",
      "iteration 6000\t accuracy: 0.961\n",
      "iteration 8000\t accuracy: 0.964\n",
      "iteration 10000\t accuracy: 0.966\n",
      "iteration 12000\t accuracy: 0.969\n",
      "iteration 14000\t accuracy: 0.968\n",
      "iteration 16000\t accuracy: 0.970\n",
      "iteration 18000\t accuracy: 0.973\n",
      "iteration 20000\t accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "test_classification(lambda x: mlp(x, [64, 10], activation_fn=tf.nn.relu), learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Siamese Network\n",
    "A siamese network is a network having two identical subnetworks that share parameters. A siamese network gives a *similarity metric* between pairs of inputs, which assigns high values to similar pairs and low values to dissimilar pairs. In MNIST, we can construct similar pairs by taking images from the same digit class, and dissimilar pairs from images from different classes. \n",
    "\n",
    "\n",
    "A sample Siamese network is presented below [Koch et al., Siamese Neural Networks for One-shot Image Recognition, ICML 2015]. Note how the two subnetworks have identical parameters:\n",
    "\n",
    "<img src=\"siamese.png\" style=\"width:480px;\">\n",
    "\n",
    "\n",
    "Given a Siamese network, we can determine the similarity between inputs $x_1$ and $x_2$, by taking the sign of the cosine similarity $\\frac{\\langle r_1,r_2\\rangle}{\\|r_1\\|\\|r_2\\|}$, where $r_1$ and $r_2$ are the hidden representations produced by the network for $x_1$ and $x_2$, respectively. \n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Build a Siamese Network in TensorFlow that receives as input two MNIST images, and decides whether they belong to the same digit category. Each subnetwork is an MLP with 2 hidden layers with 64 units, followed by a \"distance layer\" with 32 units. They will be created using the **`mlp()`** function you implemented earlier, using input argument `hidden_sizes=[64,64,32]`. The network computes the cosine similarity, as defined above, in the final output layer.\n",
    "\n",
    "2. Train and test this model, and  report your test accuracy for this task.\n",
    "\n",
    "You will need to implement the following two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Inputs: arr1 and arr2 have shape [batch_size, hidden_sizes[-1]]\n",
    "# Output: return tensor of shape [batch_size, ], the cosine \n",
    "#         similarity betwwen arr1 and arr2\n",
    "# \n",
    "# Hint: use tf.l2_normalize, tf.mul, tf.reduce_sum\n",
    "################################################################\n",
    "def cosine_similarity(arr1, arr2):\n",
    "    ###################################\n",
    "    arr1 = tf.nn.l2_normalize(arr1,dim=1)\n",
    "    arr2 = tf.nn.l2_normalize(arr2,dim=1)\n",
    "    return tf.reduce_sum(tf.multiply(arr1,arr2), axis = 1)\n",
    "    ###################################\n",
    "\n",
    "    \n",
    "#################################################################\n",
    "# Inputs: mlp_args is a dictionary of arguments to the mlp() \n",
    "#         function. \n",
    "#         Example: mlp_args = {'hidden_sizes':[64, 64, 32]}\n",
    "#################################################################\n",
    "def build_model(mlp_args):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            x1 = tf.placeholder(tf.float32, [None, 784])\n",
    "            x2 = tf.placeholder(tf.float32, [None, 784])\n",
    "            y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "            with tf.variable_scope(\"siamese\") as var_scope:\n",
    "                x_repr1 = mlp(x1, **mlp_args)  # hidden representation of x1\n",
    "                var_scope.reuse_variables()    # weight sharing! \n",
    "                x_repr2 = mlp(x2, **mlp_args)  # hidden representation of x2\n",
    "                logits = cosine_similarity(x_repr1, x_repr2) # similarity\n",
    "            ###################################\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "            y_prob = tf.nn.sigmoid(logits)\n",
    "            y_pred  = tf.nn.relu(tf.sign(logits))\n",
    "            correct_prediction = tf.equal(y_pred , y)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "            ###################################\n",
    "            \n",
    "            # define scalar: loss \n",
    "            # define vector: y_prob as sigmoid(cosine_similarity)\n",
    "            # define vector: y_pred as sign(cosine_similarity)\n",
    "            # define scalar: accuracy as the fraction of correct predictions\n",
    "            \n",
    "    return {'graph': g, 'inputs': [x1, x2, y], 'pred': y_pred, 'logits': logits,\n",
    "            'prob': y_prob, 'loss': loss, 'accuracy': accuracy, 'xrep' :x_repr1}\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code segment prepares minibatch training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data preparation\n",
    "def mnist_siamese_dataset_iterator(batch_size, dataset_name):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1 # -1 for entire dataset\n",
    "    mnist = input_data.read_data_sets('./datasets/mnist/', one_hot=True)\n",
    "    dataset = getattr(mnist, dataset_name)\n",
    "    \n",
    "    while True:\n",
    "        if batch_size > 0:\n",
    "            X1, y1 = dataset.next_batch(batch_size)\n",
    "            X2, y2 = dataset.next_batch(batch_size)\n",
    "            y = np.argmax(y1, axis=1) == np.argmax(y2, axis=1)\n",
    "            yield X1, X2, y\n",
    "        else:\n",
    "            X1 = dataset.images\n",
    "            idx = np.arange(len(X1))\n",
    "            np.random.shuffle(idx)\n",
    "            X2 = X1[idx]\n",
    "            y1 = dataset.labels\n",
    "            y2 = y1[idx]\n",
    "            y = np.argmax(y1, axis=1) == np.argmax(y2, axis=1)\n",
    "            yield X1, X2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the training.\n",
    "\n",
    "Note: if you have a good GPU, you could change `tf.device(\"/cpu:0\")` to `tf.device(\"/gpu:0\")` in your **`build_model()`** function to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Python 3\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "train iteration 0\t accuracy: 0.098, loss: 0.978\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "test iteration 0\t accuracy: 0.099, loss: 0.976\n",
      "train iteration 300\t accuracy: 0.637, loss: 0.658\n",
      "test iteration 300\t accuracy: 0.634, loss: 0.661\n",
      "train iteration 600\t accuracy: 0.684, loss: 0.648\n",
      "test iteration 600\t accuracy: 0.688, loss: 0.647\n",
      "train iteration 900\t accuracy: 0.707, loss: 0.642\n",
      "test iteration 900\t accuracy: 0.707, loss: 0.642\n",
      "train iteration 1200\t accuracy: 0.735, loss: 0.636\n",
      "test iteration 1200\t accuracy: 0.730, loss: 0.639\n",
      "train iteration 1500\t accuracy: 0.740, loss: 0.635\n",
      "test iteration 1500\t accuracy: 0.738, loss: 0.634\n",
      "train iteration 1800\t accuracy: 0.763, loss: 0.631\n",
      "test iteration 1800\t accuracy: 0.758, loss: 0.632\n",
      "train iteration 2100\t accuracy: 0.799, loss: 0.627\n",
      "test iteration 2100\t accuracy: 0.792, loss: 0.630\n",
      "train iteration 2400\t accuracy: 0.826, loss: 0.625\n",
      "test iteration 2400\t accuracy: 0.811, loss: 0.628\n",
      "train iteration 2700\t accuracy: 0.806, loss: 0.629\n",
      "test iteration 2700\t accuracy: 0.810, loss: 0.627\n",
      "train iteration 3000\t accuracy: 0.832, loss: 0.624\n",
      "test iteration 3000\t accuracy: 0.824, loss: 0.625\n",
      "train iteration 3300\t accuracy: 0.820, loss: 0.625\n",
      "test iteration 3300\t accuracy: 0.820, loss: 0.625\n",
      "train iteration 3600\t accuracy: 0.860, loss: 0.621\n",
      "test iteration 3600\t accuracy: 0.851, loss: 0.624\n",
      "train iteration 3900\t accuracy: 0.845, loss: 0.624\n",
      "test iteration 3900\t accuracy: 0.847, loss: 0.623\n",
      "train iteration 4200\t accuracy: 0.857, loss: 0.624\n",
      "test iteration 4200\t accuracy: 0.855, loss: 0.623\n",
      "train iteration 4500\t accuracy: 0.865, loss: 0.621\n",
      "test iteration 4500\t accuracy: 0.864, loss: 0.621\n",
      "train iteration 4800\t accuracy: 0.853, loss: 0.620\n",
      "test iteration 4800\t accuracy: 0.845, loss: 0.622\n",
      "train iteration 5100\t accuracy: 0.882, loss: 0.621\n",
      "test iteration 5100\t accuracy: 0.883, loss: 0.620\n",
      "train iteration 5400\t accuracy: 0.864, loss: 0.620\n",
      "test iteration 5400\t accuracy: 0.859, loss: 0.621\n",
      "train iteration 5700\t accuracy: 0.893, loss: 0.620\n",
      "test iteration 5700\t accuracy: 0.894, loss: 0.620\n",
      "train iteration 6000\t accuracy: 0.864, loss: 0.619\n",
      "test iteration 6000\t accuracy: 0.859, loss: 0.620\n",
      "train iteration 6300\t accuracy: 0.885, loss: 0.618\n",
      "test iteration 6300\t accuracy: 0.877, loss: 0.619\n",
      "train iteration 6600\t accuracy: 0.889, loss: 0.618\n",
      "test iteration 6600\t accuracy: 0.890, loss: 0.618\n",
      "train iteration 6900\t accuracy: 0.903, loss: 0.618\n",
      "test iteration 6900\t accuracy: 0.905, loss: 0.618\n",
      "train iteration 7200\t accuracy: 0.889, loss: 0.619\n",
      "test iteration 7200\t accuracy: 0.892, loss: 0.618\n",
      "train iteration 7500\t accuracy: 0.892, loss: 0.618\n",
      "test iteration 7500\t accuracy: 0.897, loss: 0.616\n",
      "train iteration 7800\t accuracy: 0.881, loss: 0.619\n",
      "test iteration 7800\t accuracy: 0.885, loss: 0.617\n",
      "train iteration 8100\t accuracy: 0.907, loss: 0.618\n",
      "test iteration 8100\t accuracy: 0.908, loss: 0.616\n",
      "train iteration 8400\t accuracy: 0.909, loss: 0.616\n",
      "test iteration 8400\t accuracy: 0.912, loss: 0.616\n",
      "train iteration 8700\t accuracy: 0.922, loss: 0.616\n",
      "test iteration 8700\t accuracy: 0.925, loss: 0.615\n",
      "train iteration 9000\t accuracy: 0.912, loss: 0.616\n",
      "test iteration 9000\t accuracy: 0.914, loss: 0.615\n",
      "train iteration 9300\t accuracy: 0.921, loss: 0.616\n",
      "test iteration 9300\t accuracy: 0.920, loss: 0.615\n",
      "train iteration 9600\t accuracy: 0.930, loss: 0.614\n",
      "test iteration 9600\t accuracy: 0.932, loss: 0.615\n",
      "train iteration 9900\t accuracy: 0.940, loss: 0.615\n",
      "test iteration 9900\t accuracy: 0.940, loss: 0.615\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from itertools import izip as zip\n",
    "except ImportError:\n",
    "    print('This is Python 3')\n",
    "\n",
    "def run_training(model_dict, train_data_iterator, test_full_iter, \n",
    "                 train_full_iter, n_iter, print_every=300):\n",
    "    with model_dict['graph'].as_default():\n",
    "        #optimizer = tf.train.AdagradOptimizer(learning_rate = 0.2)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate = 0.2)\n",
    "        train_op = optimizer.minimize(model_dict['loss'])\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for iter_i, data_batch in zip(range(n_iter), train_data_iterator):\n",
    "                batch_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(train_op, feed_dict=batch_feed_dict)\n",
    "                if iter_i % print_every == 0:\n",
    "                    print_zip_iter = zip([test_full_iter, train_full_iter], ['train', 'test'])\n",
    "                    for data_iterator, data_name in print_zip_iter:\n",
    "                        test_batch = next(data_iterator)\n",
    "                        batch_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['accuracy'], model_dict['loss']]\n",
    "                        acc_value, loss_val = sess.run(to_compute, batch_feed_dict)\n",
    "                        fmt = (iter_i, acc_value, loss_val)\n",
    "                        print(data_name, 'iteration %d\\t accuracy: %.3f, loss: %.3f'%fmt)\n",
    "\n",
    "train_data_iterator = mnist_siamese_dataset_iterator(100, 'train')\n",
    "test_full_iter = mnist_siamese_dataset_iterator(-1, 'test')\n",
    "train_full_iter = mnist_siamese_dataset_iterator(-1, 'train')\n",
    "\n",
    "mlp_args = {'hidden_sizes':[64, 64, 32] , 'activation_fn' :tf.nn.relu}\n",
    "model = build_model(mlp_args)\n",
    "run_training(model, train_data_iterator, test_full_iter, train_full_iter,  n_iter = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Variants\n",
    "\n",
    "(30 points)\n",
    "\n",
    "Tasks:\n",
    " \n",
    " 1. Create an improved version of the model in part 2. You are welcome to use techniques covered in class, including: increasing network depth, varying activation functions, learning rates, optimizers, cost functions, regularization etc. \n",
    " \n",
    " 2. Report the test accuracy.\n",
    "\n",
    "Note: you should **not** use convolutional layers for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Python 3\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "train iteration 0\t accuracy: 0.103, loss: 1.024\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "test iteration 0\t accuracy: 0.102, loss: 1.027\n",
      "train iteration 500\t accuracy: 0.735, loss: 0.649\n",
      "test iteration 500\t accuracy: 0.738, loss: 0.647\n",
      "train iteration 1000\t accuracy: 0.794, loss: 0.638\n",
      "test iteration 1000\t accuracy: 0.788, loss: 0.641\n",
      "train iteration 1500\t accuracy: 0.808, loss: 0.635\n",
      "test iteration 1500\t accuracy: 0.816, loss: 0.632\n",
      "train iteration 2000\t accuracy: 0.839, loss: 0.630\n",
      "test iteration 2000\t accuracy: 0.836, loss: 0.629\n",
      "train iteration 2500\t accuracy: 0.852, loss: 0.623\n",
      "test iteration 2500\t accuracy: 0.847, loss: 0.627\n",
      "train iteration 3000\t accuracy: 0.885, loss: 0.620\n",
      "test iteration 3000\t accuracy: 0.875, loss: 0.625\n",
      "train iteration 3500\t accuracy: 0.889, loss: 0.621\n",
      "test iteration 3500\t accuracy: 0.883, loss: 0.621\n",
      "train iteration 4000\t accuracy: 0.893, loss: 0.621\n",
      "test iteration 4000\t accuracy: 0.897, loss: 0.622\n",
      "train iteration 4500\t accuracy: 0.896, loss: 0.620\n",
      "test iteration 4500\t accuracy: 0.896, loss: 0.620\n",
      "train iteration 5000\t accuracy: 0.903, loss: 0.624\n",
      "test iteration 5000\t accuracy: 0.907, loss: 0.620\n",
      "train iteration 5500\t accuracy: 0.921, loss: 0.621\n",
      "test iteration 5500\t accuracy: 0.922, loss: 0.620\n",
      "train iteration 6000\t accuracy: 0.927, loss: 0.619\n",
      "test iteration 6000\t accuracy: 0.929, loss: 0.619\n",
      "train iteration 6500\t accuracy: 0.923, loss: 0.620\n",
      "test iteration 6500\t accuracy: 0.922, loss: 0.620\n",
      "train iteration 7000\t accuracy: 0.930, loss: 0.619\n",
      "test iteration 7000\t accuracy: 0.931, loss: 0.617\n",
      "train iteration 7500\t accuracy: 0.937, loss: 0.617\n",
      "test iteration 7500\t accuracy: 0.939, loss: 0.618\n",
      "train iteration 8000\t accuracy: 0.932, loss: 0.621\n",
      "test iteration 8000\t accuracy: 0.934, loss: 0.618\n",
      "train iteration 8500\t accuracy: 0.927, loss: 0.617\n",
      "test iteration 8500\t accuracy: 0.927, loss: 0.618\n",
      "train iteration 9000\t accuracy: 0.932, loss: 0.618\n",
      "test iteration 9000\t accuracy: 0.934, loss: 0.617\n",
      "train iteration 9500\t accuracy: 0.939, loss: 0.622\n",
      "test iteration 9500\t accuracy: 0.942, loss: 0.618\n",
      "train iteration 10000\t accuracy: 0.922, loss: 0.617\n",
      "test iteration 10000\t accuracy: 0.922, loss: 0.617\n",
      "train iteration 10500\t accuracy: 0.936, loss: 0.620\n",
      "test iteration 10500\t accuracy: 0.942, loss: 0.616\n",
      "train iteration 11000\t accuracy: 0.948, loss: 0.613\n",
      "test iteration 11000\t accuracy: 0.950, loss: 0.615\n",
      "train iteration 11500\t accuracy: 0.939, loss: 0.615\n",
      "test iteration 11500\t accuracy: 0.939, loss: 0.614\n",
      "train iteration 12000\t accuracy: 0.956, loss: 0.614\n",
      "test iteration 12000\t accuracy: 0.958, loss: 0.614\n",
      "train iteration 12500\t accuracy: 0.951, loss: 0.614\n",
      "test iteration 12500\t accuracy: 0.951, loss: 0.614\n",
      "train iteration 13000\t accuracy: 0.956, loss: 0.617\n",
      "test iteration 13000\t accuracy: 0.960, loss: 0.615\n",
      "train iteration 13500\t accuracy: 0.963, loss: 0.615\n",
      "test iteration 13500\t accuracy: 0.963, loss: 0.614\n",
      "train iteration 14000\t accuracy: 0.963, loss: 0.614\n",
      "test iteration 14000\t accuracy: 0.968, loss: 0.613\n",
      "train iteration 14500\t accuracy: 0.966, loss: 0.614\n",
      "test iteration 14500\t accuracy: 0.969, loss: 0.613\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from itertools import izip as zip\n",
    "except ImportError:\n",
    "    print('This is Python 3')\n",
    "\n",
    "def run_training(model_dict, train_data_iterator, test_full_iter, \n",
    "                 train_full_iter, n_iter, print_every=500):\n",
    "    with model_dict['graph'].as_default():\n",
    "        #optimizer = tf.train.AdagradOptimizer(learning_rate = 0.2)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate = 0.2)\n",
    "        train_op = optimizer.minimize(model_dict['loss'])\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for iter_i, data_batch in zip(range(n_iter), train_data_iterator):\n",
    "                batch_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(train_op, feed_dict=batch_feed_dict)\n",
    "                if iter_i % print_every == 0:\n",
    "                    print_zip_iter = zip([test_full_iter, train_full_iter], ['train', 'test'])\n",
    "                    for data_iterator, data_name in print_zip_iter:\n",
    "                        test_batch = next(data_iterator)\n",
    "                        batch_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['accuracy'], model_dict['loss']]\n",
    "                        acc_value, loss_val = sess.run(to_compute, batch_feed_dict)\n",
    "                        fmt = (iter_i, acc_value, loss_val)\n",
    "                        print(data_name, 'iteration %d\\t accuracy: %.3f, loss: %.3f'%fmt)\n",
    "\n",
    "train_data_iterator = mnist_siamese_dataset_iterator(100, 'train')\n",
    "test_full_iter = mnist_siamese_dataset_iterator(-1, 'test')\n",
    "train_full_iter = mnist_siamese_dataset_iterator(-1, 'train')\n",
    "\n",
    "mlp_args = {'hidden_sizes':[64, 64,32, 32] , 'activation_fn' :tf.nn.relu}\n",
    "model = build_model(mlp_args)\n",
    "run_training(model, train_data_iterator, test_full_iter, train_full_iter,  n_iter = 15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deeper Network:**\n",
    "As we have seen above, when we make our network deeper our Acc is gonna be better and it is actually obvious because wwhen we have more hidden layer our performance is going to be better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Python 3\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "train iteration 0\t accuracy: 0.102, loss: 1.039\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "test iteration 0\t accuracy: 0.100, loss: 1.039\n",
      "train iteration 500\t accuracy: 0.654, loss: 0.654\n",
      "test iteration 500\t accuracy: 0.658, loss: 0.654\n",
      "train iteration 1000\t accuracy: 0.715, loss: 0.643\n",
      "test iteration 1000\t accuracy: 0.716, loss: 0.642\n",
      "train iteration 1500\t accuracy: 0.734, loss: 0.636\n",
      "test iteration 1500\t accuracy: 0.733, loss: 0.636\n",
      "train iteration 2000\t accuracy: 0.766, loss: 0.629\n",
      "test iteration 2000\t accuracy: 0.757, loss: 0.633\n",
      "train iteration 2500\t accuracy: 0.790, loss: 0.628\n",
      "test iteration 2500\t accuracy: 0.780, loss: 0.630\n",
      "train iteration 3000\t accuracy: 0.799, loss: 0.627\n",
      "test iteration 3000\t accuracy: 0.795, loss: 0.627\n",
      "train iteration 3500\t accuracy: 0.830, loss: 0.625\n",
      "test iteration 3500\t accuracy: 0.825, loss: 0.625\n",
      "train iteration 4000\t accuracy: 0.839, loss: 0.625\n",
      "test iteration 4000\t accuracy: 0.839, loss: 0.623\n",
      "train iteration 4500\t accuracy: 0.858, loss: 0.622\n",
      "test iteration 4500\t accuracy: 0.853, loss: 0.623\n",
      "train iteration 5000\t accuracy: 0.877, loss: 0.624\n",
      "test iteration 5000\t accuracy: 0.876, loss: 0.623\n",
      "train iteration 5500\t accuracy: 0.882, loss: 0.621\n",
      "test iteration 5500\t accuracy: 0.883, loss: 0.620\n",
      "train iteration 6000\t accuracy: 0.912, loss: 0.618\n",
      "test iteration 6000\t accuracy: 0.904, loss: 0.619\n",
      "train iteration 6500\t accuracy: 0.921, loss: 0.615\n",
      "test iteration 6500\t accuracy: 0.915, loss: 0.617\n",
      "train iteration 7000\t accuracy: 0.920, loss: 0.616\n",
      "test iteration 7000\t accuracy: 0.916, loss: 0.617\n",
      "train iteration 7500\t accuracy: 0.931, loss: 0.616\n",
      "test iteration 7500\t accuracy: 0.924, loss: 0.617\n",
      "train iteration 8000\t accuracy: 0.931, loss: 0.616\n",
      "test iteration 8000\t accuracy: 0.931, loss: 0.616\n",
      "train iteration 8500\t accuracy: 0.934, loss: 0.616\n",
      "test iteration 8500\t accuracy: 0.935, loss: 0.616\n",
      "train iteration 9000\t accuracy: 0.944, loss: 0.618\n",
      "test iteration 9000\t accuracy: 0.942, loss: 0.615\n",
      "train iteration 9500\t accuracy: 0.947, loss: 0.614\n",
      "test iteration 9500\t accuracy: 0.944, loss: 0.614\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from itertools import izip as zip\n",
    "except ImportError:\n",
    "    print('This is Python 3')\n",
    "\n",
    "def run_training(model_dict, train_data_iterator, test_full_iter, \n",
    "                 train_full_iter, n_iter, print_every=500):\n",
    "    with model_dict['graph'].as_default():\n",
    "        #optimizer = tf.train.AdagradOptimizer(learning_rate = 0.2)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate = 0.2)\n",
    "        train_op = optimizer.minimize(model_dict['loss'])\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for iter_i, data_batch in zip(range(n_iter), train_data_iterator):\n",
    "                batch_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(train_op, feed_dict=batch_feed_dict)\n",
    "                if iter_i % print_every == 0:\n",
    "                    print_zip_iter = zip([test_full_iter, train_full_iter], ['train', 'test'])\n",
    "                    for data_iterator, data_name in print_zip_iter:\n",
    "                        test_batch = next(data_iterator)\n",
    "                        batch_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['accuracy'], model_dict['loss']]\n",
    "                        acc_value, loss_val = sess.run(to_compute, batch_feed_dict)\n",
    "                        fmt = (iter_i, acc_value, loss_val)\n",
    "                        print(data_name, 'iteration %d\\t accuracy: %.3f, loss: %.3f'%fmt)\n",
    "\n",
    "train_data_iterator = mnist_siamese_dataset_iterator(100, 'train')\n",
    "test_full_iter = mnist_siamese_dataset_iterator(-1, 'test')\n",
    "train_full_iter = mnist_siamese_dataset_iterator(-1, 'train')\n",
    "\n",
    "mlp_args = {'hidden_sizes':[64, 64, 32] , 'activation_fn' :tf.nn.softplus}\n",
    "model = build_model(mlp_args)\n",
    "run_training(model, train_data_iterator, test_full_iter, train_full_iter,  n_iter = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing Activation Function:**\n",
    "In this part I just replace relu with softplus and then I get almostly the same result with relu(as I have explained before in pset2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "train iteration 0\t accuracy: 0.119, loss: 0.869\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "test iteration 0\t accuracy: 0.121, loss: 0.865\n",
      "train iteration 500\t accuracy: 0.668, loss: 0.656\n",
      "test iteration 500\t accuracy: 0.665, loss: 0.655\n",
      "train iteration 1000\t accuracy: 0.733, loss: 0.648\n",
      "test iteration 1000\t accuracy: 0.738, loss: 0.642\n",
      "train iteration 1500\t accuracy: 0.773, loss: 0.638\n",
      "test iteration 1500\t accuracy: 0.768, loss: 0.639\n",
      "train iteration 2000\t accuracy: 0.796, loss: 0.634\n",
      "test iteration 2000\t accuracy: 0.792, loss: 0.636\n",
      "train iteration 2500\t accuracy: 0.845, loss: 0.630\n",
      "test iteration 2500\t accuracy: 0.838, loss: 0.632\n",
      "train iteration 3000\t accuracy: 0.832, loss: 0.630\n",
      "test iteration 3000\t accuracy: 0.825, loss: 0.631\n",
      "train iteration 3500\t accuracy: 0.846, loss: 0.627\n",
      "test iteration 3500\t accuracy: 0.838, loss: 0.629\n",
      "train iteration 4000\t accuracy: 0.860, loss: 0.627\n",
      "test iteration 4000\t accuracy: 0.855, loss: 0.629\n",
      "train iteration 4500\t accuracy: 0.847, loss: 0.625\n",
      "test iteration 4500\t accuracy: 0.841, loss: 0.627\n",
      "train iteration 5000\t accuracy: 0.877, loss: 0.625\n",
      "test iteration 5000\t accuracy: 0.871, loss: 0.626\n",
      "train iteration 5500\t accuracy: 0.871, loss: 0.622\n",
      "test iteration 5500\t accuracy: 0.867, loss: 0.624\n",
      "train iteration 6000\t accuracy: 0.873, loss: 0.625\n",
      "test iteration 6000\t accuracy: 0.870, loss: 0.624\n",
      "train iteration 6500\t accuracy: 0.868, loss: 0.624\n",
      "test iteration 6500\t accuracy: 0.869, loss: 0.623\n",
      "train iteration 7000\t accuracy: 0.878, loss: 0.623\n",
      "test iteration 7000\t accuracy: 0.874, loss: 0.624\n",
      "train iteration 7500\t accuracy: 0.895, loss: 0.623\n",
      "test iteration 7500\t accuracy: 0.890, loss: 0.622\n",
      "train iteration 8000\t accuracy: 0.891, loss: 0.624\n",
      "test iteration 8000\t accuracy: 0.888, loss: 0.622\n",
      "train iteration 8500\t accuracy: 0.883, loss: 0.622\n",
      "test iteration 8500\t accuracy: 0.886, loss: 0.620\n",
      "train iteration 9000\t accuracy: 0.902, loss: 0.620\n",
      "test iteration 9000\t accuracy: 0.904, loss: 0.620\n",
      "train iteration 9500\t accuracy: 0.902, loss: 0.621\n",
      "test iteration 9500\t accuracy: 0.904, loss: 0.620\n"
     ]
    }
   ],
   "source": [
    "def run_training(model_dict, train_data_iterator, test_full_iter, \n",
    "                 train_full_iter, n_iter, print_every=500):\n",
    "    with model_dict['graph'].as_default():\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate = 0.2)\n",
    "        #optimizer = tf.train.AdamOptimizer()\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate = 0.2)\n",
    "        train_op = optimizer.minimize(model_dict['loss'])\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for iter_i, data_batch in zip(range(n_iter), train_data_iterator):\n",
    "                batch_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(train_op, feed_dict=batch_feed_dict)\n",
    "                if iter_i % print_every == 0:\n",
    "                    print_zip_iter = zip([test_full_iter, train_full_iter], ['train', 'test'])\n",
    "                    for data_iterator, data_name in print_zip_iter:\n",
    "                        test_batch = next(data_iterator)\n",
    "                        batch_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['accuracy'], model_dict['loss']]\n",
    "                        acc_value, loss_val = sess.run(to_compute, batch_feed_dict)\n",
    "                        fmt = (iter_i, acc_value, loss_val)\n",
    "                        print(data_name, 'iteration %d\\t accuracy: %.3f, loss: %.3f'%fmt)\n",
    "\n",
    "train_data_iterator = mnist_siamese_dataset_iterator(100, 'train')\n",
    "test_full_iter = mnist_siamese_dataset_iterator(-1, 'test')\n",
    "train_full_iter = mnist_siamese_dataset_iterator(-1, 'train')\n",
    "\n",
    "mlp_args = {'hidden_sizes':[64, 64, 32] , 'activation_fn' :tf.nn.relu}\n",
    "model = build_model(mlp_args)\n",
    "run_training(model, train_data_iterator, test_full_iter, train_full_iter,  n_iter = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Changing Optimizer: **\n",
    "In this part we replace AdamOptimizer() with AdagradOptimizer() and we are seeing that it works a little bit worse and it converges a bit slower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Python 3\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "train iteration 0\t accuracy: 0.103, loss: 1.018\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "test iteration 0\t accuracy: 0.099, loss: 1.021\n",
      "train iteration 500\t accuracy: 0.684, loss: 0.648\n",
      "test iteration 500\t accuracy: 0.680, loss: 0.649\n",
      "train iteration 1000\t accuracy: 0.732, loss: 0.640\n",
      "test iteration 1000\t accuracy: 0.732, loss: 0.638\n",
      "train iteration 1500\t accuracy: 0.788, loss: 0.635\n",
      "test iteration 1500\t accuracy: 0.793, loss: 0.632\n",
      "train iteration 2000\t accuracy: 0.822, loss: 0.629\n",
      "test iteration 2000\t accuracy: 0.823, loss: 0.629\n",
      "train iteration 2500\t accuracy: 0.829, loss: 0.626\n",
      "test iteration 2500\t accuracy: 0.827, loss: 0.627\n",
      "train iteration 3000\t accuracy: 0.851, loss: 0.622\n",
      "test iteration 3000\t accuracy: 0.846, loss: 0.625\n",
      "train iteration 3500\t accuracy: 0.860, loss: 0.624\n",
      "test iteration 3500\t accuracy: 0.855, loss: 0.624\n",
      "train iteration 4000\t accuracy: 0.883, loss: 0.621\n",
      "test iteration 4000\t accuracy: 0.882, loss: 0.621\n",
      "train iteration 4500\t accuracy: 0.882, loss: 0.621\n",
      "test iteration 4500\t accuracy: 0.885, loss: 0.621\n",
      "train iteration 5000\t accuracy: 0.892, loss: 0.621\n",
      "test iteration 5000\t accuracy: 0.893, loss: 0.620\n",
      "train iteration 5500\t accuracy: 0.881, loss: 0.620\n",
      "test iteration 5500\t accuracy: 0.885, loss: 0.619\n",
      "train iteration 6000\t accuracy: 0.893, loss: 0.620\n",
      "test iteration 6000\t accuracy: 0.893, loss: 0.620\n",
      "train iteration 6500\t accuracy: 0.901, loss: 0.618\n",
      "test iteration 6500\t accuracy: 0.907, loss: 0.619\n",
      "train iteration 7000\t accuracy: 0.898, loss: 0.621\n",
      "test iteration 7000\t accuracy: 0.900, loss: 0.618\n",
      "train iteration 7500\t accuracy: 0.907, loss: 0.619\n",
      "test iteration 7500\t accuracy: 0.908, loss: 0.617\n",
      "train iteration 8000\t accuracy: 0.914, loss: 0.622\n",
      "test iteration 8000\t accuracy: 0.922, loss: 0.618\n",
      "train iteration 8500\t accuracy: 0.911, loss: 0.617\n",
      "test iteration 8500\t accuracy: 0.915, loss: 0.616\n",
      "train iteration 9000\t accuracy: 0.919, loss: 0.616\n",
      "test iteration 9000\t accuracy: 0.921, loss: 0.616\n",
      "train iteration 9500\t accuracy: 0.921, loss: 0.615\n",
      "test iteration 9500\t accuracy: 0.919, loss: 0.616\n",
      "train iteration 10000\t accuracy: 0.928, loss: 0.616\n",
      "test iteration 10000\t accuracy: 0.929, loss: 0.615\n",
      "train iteration 10500\t accuracy: 0.932, loss: 0.615\n",
      "test iteration 10500\t accuracy: 0.935, loss: 0.615\n",
      "train iteration 11000\t accuracy: 0.950, loss: 0.614\n",
      "test iteration 11000\t accuracy: 0.951, loss: 0.614\n",
      "train iteration 11500\t accuracy: 0.950, loss: 0.616\n",
      "test iteration 11500\t accuracy: 0.953, loss: 0.614\n",
      "train iteration 12000\t accuracy: 0.946, loss: 0.614\n",
      "test iteration 12000\t accuracy: 0.951, loss: 0.614\n",
      "train iteration 12500\t accuracy: 0.942, loss: 0.615\n",
      "test iteration 12500\t accuracy: 0.949, loss: 0.611\n",
      "train iteration 13000\t accuracy: 0.947, loss: 0.615\n",
      "test iteration 13000\t accuracy: 0.950, loss: 0.613\n",
      "train iteration 13500\t accuracy: 0.955, loss: 0.614\n",
      "test iteration 13500\t accuracy: 0.960, loss: 0.612\n",
      "train iteration 14000\t accuracy: 0.955, loss: 0.611\n",
      "test iteration 14000\t accuracy: 0.960, loss: 0.613\n",
      "train iteration 14500\t accuracy: 0.930, loss: 0.613\n",
      "test iteration 14500\t accuracy: 0.934, loss: 0.613\n"
     ]
    }
   ],
   "source": [
    "def build_model(mlp_args):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            x1 = tf.placeholder(tf.float32, [None, 784])\n",
    "            x2 = tf.placeholder(tf.float32, [None, 784])\n",
    "            y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "            with tf.variable_scope(\"siamese\") as var_scope:\n",
    "                x_repr1 = mlp(x1, **mlp_args)  # hidden representation of x1\n",
    "                var_scope.reuse_variables()    # weight sharing! \n",
    "                x_repr2 = mlp(x2, **mlp_args)  # hidden representation of x2\n",
    "                logits = cosine_similarity(x_repr1, x_repr2) # similarity\n",
    "            ###################################\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "            loss = loss + 0.01 * sum(reg_losses)\n",
    "            y_prob = tf.nn.sigmoid(logits)\n",
    "            y_pred  = tf.nn.relu(tf.sign(logits))\n",
    "            correct_prediction = tf.equal(y_pred , y)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "            ###################################\n",
    "            \n",
    "            # define scalar: loss \n",
    "            # define vector: y_prob as sigmoid(cosine_similarity)\n",
    "            # define vector: y_pred as sign(cosine_similarity)\n",
    "            # define scalar: accuracy as the fraction of correct predictions\n",
    "            \n",
    "    return {'graph': g, 'inputs': [x1, x2, y], 'pred': y_pred, 'logits': logits,\n",
    "            'prob': y_prob, 'loss': loss, 'accuracy': accuracy, 'xrep' :x_repr1}\n",
    "\n",
    "try:\n",
    "    from itertools import izip as zip\n",
    "except ImportError:\n",
    "    print('This is Python 3')\n",
    "\n",
    "def run_training(model_dict, train_data_iterator, test_full_iter, \n",
    "                 train_full_iter, n_iter, print_every=500):\n",
    "    with model_dict['graph'].as_default():\n",
    "        #optimizer = tf.train.AdagradOptimizer(learning_rate = 0.2)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate = 0.2)\n",
    "        train_op = optimizer.minimize(model_dict['loss'])\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for iter_i, data_batch in zip(range(n_iter), train_data_iterator):\n",
    "                batch_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(train_op, feed_dict=batch_feed_dict)\n",
    "                if iter_i % print_every == 0:\n",
    "                    print_zip_iter = zip([test_full_iter, train_full_iter], ['train', 'test'])\n",
    "                    for data_iterator, data_name in print_zip_iter:\n",
    "                        test_batch = next(data_iterator)\n",
    "                        batch_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['accuracy'], model_dict['loss']]\n",
    "                        acc_value, loss_val = sess.run(to_compute, batch_feed_dict)\n",
    "                        fmt = (iter_i, acc_value, loss_val)\n",
    "                        print(data_name, 'iteration %d\\t accuracy: %.3f, loss: %.3f'%fmt)\n",
    "\n",
    "train_data_iterator = mnist_siamese_dataset_iterator(100, 'train')\n",
    "test_full_iter = mnist_siamese_dataset_iterator(-1, 'test')\n",
    "train_full_iter = mnist_siamese_dataset_iterator(-1, 'train')\n",
    "\n",
    "mlp_args = {'hidden_sizes':[64, 64, 32] , 'activation_fn' :tf.nn.relu}\n",
    "model = build_model(mlp_args)\n",
    "run_training(model, train_data_iterator, test_full_iter, train_full_iter,  n_iter = 15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regulirization:** \n",
    "Here we add regulirization to our loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualize learned features\n",
    "(25 points)\n",
    "\n",
    "Once a neural network is trained, we can think of the last layer as performing prediction, and the activations from layer before  become the input \"features\" to the predictor. If the neural network is performing good predictions, then this means that these activations encode useful features that are 1) representative of the input, and 2) discriminative for the prediction task. Activations at a selected layer could then be used as generic feature encodings of the input. \n",
    "\n",
    "We  expect a \"good\" feature encoding scheme to group similar inputs together in the feature encoding space. To check that, we can visualize the features in 2-dimensional space and check if similar examples (in this case, sharing the same labels) are close together. \n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. In TensorFlow, extract 32-dimensional features from the **distance layer** of your trained Siamese network, for the MNIST **test set**.\n",
    "2. Reduce the dimensionality of your deep features to 2 using [t-SNE embedding](https://lvdmaaten.github.io/tsne/). You may use [this fast implementation](https://github.com/lvdmaaten/bhtsne/) with correct attribution. You could alternatively use [sklearn.manifold.TSNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), but please take note that it might be slow.\n",
    "3. Visualize the 2-dimensional embeddings. If your extracted features are good, data points representing a specific digit should appear within a compact cluster. In the example below, each color corresponds to a digit class.\n",
    "\n",
    " <img src=\"tsne.png\" style=\"width:480px;\">\n",
    " \n",
    "For this part, the starter code is in minimalist fashion. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bhtsne\n",
    "import pylab as pl\n",
    "\n",
    "def visualize(features):\n",
    "    tsne2 = bhtsne.run_bh_tsne(features[1:10,:], initial_dims=features.shape[1])\n",
    "    pl.plot(tsne2[0,:], tnse2[1,:])\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "train iteration 0\t accuracy: 0.180, loss: 0.821\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "test iteration 0\t accuracy: 0.179, loss: 0.821\n",
      "train iteration 300\t accuracy: 0.627, loss: 0.661\n",
      "test iteration 300\t accuracy: 0.630, loss: 0.659\n",
      "train iteration 600\t accuracy: 0.663, loss: 0.653\n",
      "test iteration 600\t accuracy: 0.664, loss: 0.651\n",
      "train iteration 900\t accuracy: 0.664, loss: 0.650\n",
      "test iteration 900\t accuracy: 0.670, loss: 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siaa/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please run this program directly from python and not from ipython or jupyter.\n",
      "This is an issue due to asynchronous error handling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 412, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 628, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 634, in _abort_queue\n",
      "    idents,msg = self.session.recv(stream, zmq.NOBLOCK, content=True)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\", line 739, in recv\n",
      "    msg_list = socket.recv_multipart(mode, copy=copy)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\", line 395, in recv_multipart\n",
      "    parts = [self.recv(flags, copy=copy, track=track)]\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 693, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7683)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 727, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7460)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 145, in zmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:2344)\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 15, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:9655)\n",
      "  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 946, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 885, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1157, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1126, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1096, in _path_importer_cache\n",
      "KeyboardInterrupt\n",
      "\n",
      "ERROR:tornado.general:Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 412, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 628, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 634, in _abort_queue\n",
      "    idents,msg = self.session.recv(stream, zmq.NOBLOCK, content=True)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\", line 739, in recv\n",
      "    msg_list = socket.recv_multipart(mode, copy=copy)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\", line 395, in recv_multipart\n",
      "    parts = [self.recv(flags, copy=copy, track=track)]\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 693, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7683)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 727, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7460)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 145, in zmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:2344)\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 15, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:9655)\n",
      "  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 946, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 885, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1157, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1126, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1096, in _path_importer_cache\n",
      "KeyboardInterruptERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-40-348e2dd0cfac>\", line 31, in <module>\n",
      "    run_training(model, train_data_iterator, test_full_iter, train_full_iter,  n_iter = 1000)\n",
      "  File \"<ipython-input-40-348e2dd0cfac>\", line 23, in run_training\n",
      "    visualize(features)\n",
      "  File \"<ipython-input-39-282f7cebdea1>\", line 5, in visualize\n",
      "    tsne2 = bhtsne.run_bh_tsne(features[1:10,:], initial_dims=features.shape[1])\n",
      "  File \"/home/siaa/Downloads/MehrnooshDeepLearn/cs591s2-master/bhtsne.py\", line 214, in run_bh_tsne\n",
      "    for result in bh_tsne(tmp_dir_path, verbose):\n",
      "  File \"/home/siaa/Downloads/MehrnooshDeepLearn/cs591s2-master/bhtsne.py\", line 156, in bh_tsne\n",
      "    'refer to the bh_tsne output for further details')\n",
      "AssertionError: ERROR: Call to bh_tsne exited with a non-zero return code exit status, please enable verbose mode and refer to the bh_tsne output for further details\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AssertionError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/inspect.py\", line 1454, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/inspect.py\", line 1415, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/siaa/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 229, in findsource\n",
      "    if pmatch(lines[lnum]):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "ERROR: Call to bh_tsne exited with a non-zero return code exit status, please enable verbose mode and refer to the bh_tsne output for further details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "def run_training(model_dict, train_data_iterator, test_full_iter, \n",
    "                 train_full_iter, n_iter, print_every=300):\n",
    "    with model_dict['graph'].as_default():\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate = 0.2)\n",
    "        #optimizer = tf.train.AdamOptimizer()\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate = 0.2)\n",
    "        train_op = optimizer.minimize(model_dict['loss'])\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for iter_i, data_batch in zip(range(n_iter), train_data_iterator):\n",
    "                batch_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(train_op, feed_dict=batch_feed_dict)\n",
    "                if iter_i % print_every == 0:\n",
    "                    print_zip_iter = zip([test_full_iter, train_full_iter], ['train', 'test'])\n",
    "                    for data_iterator, data_name in print_zip_iter:\n",
    "                        test_batch = next(data_iterator)\n",
    "                        batch_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['accuracy'], model_dict['loss']]\n",
    "                        acc_value, loss_val = sess.run(to_compute, batch_feed_dict)\n",
    "                        fmt = (iter_i, acc_value, loss_val)\n",
    "                        print(data_name, 'iteration %d\\t accuracy: %.3f, loss: %.3f'%fmt)\n",
    "                        features = sess.run(model_dict['xrep'], batch_feed_dict)\n",
    "            visualize(features)\n",
    "\n",
    "train_data_iterator = mnist_siamese_dataset_iterator(100, 'train')\n",
    "test_full_iter = mnist_siamese_dataset_iterator(-1, 'test')\n",
    "train_full_iter = mnist_siamese_dataset_iterator(-1, 'train')\n",
    "\n",
    "mlp_args = {'hidden_sizes':[64, 64, 32] , 'activation_fn' :tf.nn.relu}\n",
    "model = build_model(mlp_args)\n",
    "run_training(model, train_data_iterator, test_full_iter, train_full_iter,  n_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
